{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have (article text, article id) tuples for each neighborhood\n",
    "# turn all articles into spacy-processible list of documents, but maintain the tuples\n",
    "    # the result would be a tuple of a list of documents and the article id\n",
    "# for each list, extract out all people, feed their last names to ethnicolr to predict races and then tag the result with the article id\n",
    "# for each neighborhood, get percentage of races\n",
    "# journalism team will compare results with U.S. Census data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import en_core_web_md\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load medium English model in case we need to work with vectors\n",
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Neighborhood_Separated_Articles/2014.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_neighborhoods = ['dorchester', 'roxbury', 'mattapan', 'hyde_park']\n",
    "white_neighborhoods = ['fenway', 'beacon_hill', 'downtown', 'south_boston', 'east_boston', 'back_bay', 'jamaica_plain',\n",
    "                      'south_end', 'charlestown', 'brighton', 'allston', 'west_end', 'roslindale', 'north_end',\n",
    "                      'mission_hill', 'harbor_islands', 'longwood_medical_area', 'west_roxbury']\n",
    "df = df.fillna(\"('no article', 'no_id')\")\n",
    "df['dorchester'] = df['dorchester'].apply(ast.literal_eval)\n",
    "df['roxbury'] = df['roxbury'].apply(ast.literal_eval)\n",
    "df['mattapan'] = df['mattapan'].apply(ast.literal_eval)\n",
    "df['hyde_park'] = df['hyde_park'].apply(ast.literal_eval)\n",
    "df['fenway'] = df['fenway'].apply(ast.literal_eval)\n",
    "df['beacon_hill'] = df['beacon_hill'].apply(ast.literal_eval)\n",
    "df['downtown'] = df['downtown'].apply(ast.literal_eval)\n",
    "df['south_boston'] = df['south_boston'].apply(ast.literal_eval)\n",
    "df['east_boston'] = df['east_boston'].apply(ast.literal_eval)\n",
    "df['back_bay'] = df['back_bay'].apply(ast.literal_eval)\n",
    "df['jamaica_plain'] = df['jamaica_plain'].apply(ast.literal_eval)\n",
    "df['south_end'] = df['south_end'].apply(ast.literal_eval)\n",
    "df['charlestown'] = df['charlestown'].apply(ast.literal_eval)\n",
    "df['brighton'] = df['brighton'].apply(ast.literal_eval)\n",
    "df['allston'] = df['allston'].apply(ast.literal_eval)\n",
    "df['west_end'] = df['west_end'].apply(ast.literal_eval)\n",
    "df['roslindale'] = df['roslindale'].apply(ast.literal_eval)\n",
    "df['north_end'] = df['north_end'].apply(ast.literal_eval)\n",
    "df['mission_hill'] = df['mission_hill'].apply(ast.literal_eval)\n",
    "df['harbor_islands'] = df['harbor_islands'].apply(ast.literal_eval)\n",
    "df['longwood_medical_area'] = df['longwood_medical_area'].apply(ast.literal_eval)\n",
    "df['west_roxbury'] = df['west_roxbury'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_chars = [\"!\",'\"',\"#\",\"%\",\"&\",\"'\",\"(\",\")\", \"*\",\"+\",\",\",\n",
    "                  \"-\",\".\",\"/\",\":\",\";\",\"<\", \"=\",\">\",\"?\",\"@\",\"[\",\n",
    "                  \"\\\\\",\"]\",\"^\",\"_\", \"`\",\"{\",\"|\",\"}\",\"~\",\"–\", \n",
    "                  \"\\xc2\", \"\\xa0\", \"\\x80\", \"\\x9c\", \"\\x99\", \"\\x94\", \n",
    "                  \"\\xad\", \"\\xe2\", \"\\x9d\", \"\\n\"]\n",
    "\n",
    "df = df.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "#for char in spec_chars:\n",
    "#    df['text'] = df['text'].str.strip()\n",
    "#    df['text'] = df['text'].str.replace(char, ' ')\n",
    "       \n",
    "# access each column separately\n",
    "for i in range(len(df.index)):\n",
    "    for col in df.columns:\n",
    "        for char in spec_chars:\n",
    "            try:\n",
    "                df.loc[i, col][0] = df.loc[i, col][0].str.strip()\n",
    "                df.loc[i, col][0] = df.loc[i, col][0].str.replace(char, ' ')\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 22)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyde_park DONE\n",
      "beacon_hill DONE\n",
      "south_boston DONE\n",
      "jamaica_plain DONE\n",
      "east_boston DONE\n",
      "south_end DONE\n",
      "back_bay DONE\n",
      "north_end DONE\n",
      "west_roxbury DONE\n",
      "mission_hill DONE\n",
      "harbor_islands DONE\n",
      "west_end DONE\n",
      "longwood_medical_area DONE\n",
      "dorchester DONE\n",
      "roxbury DONE\n",
      "downtown DONE\n",
      "fenway DONE\n",
      "mattapan DONE\n",
      "brighton DONE\n",
      "charlestown DONE\n",
      "roslindale DONE\n",
      "allston DONE\n"
     ]
    }
   ],
   "source": [
    "articles = {'hyde_park': [], 'beacon_hill': [], 'south_boston': [], 'jamaica_plain': [], 'east_boston': [],\n",
    "                'south_end': [], 'back_bay': [], 'north_end': [], 'west_roxbury': [], 'mission_hill': [],\n",
    "                'harbor_islands': [], 'west_end': [], 'longwood_medical_area': [],\n",
    "                'dorchester': [], 'roxbury': [], 'downtown': [], 'fenway': [], 'mattapan': [], 'brighton': [],\n",
    "                'charlestown': [], 'roslindale': [], 'allston': []}\n",
    "for sub_neighborhood in articles.keys():\n",
    "    for i in range(df.shape[0]):\n",
    "        if type(df.loc[i, sub_neighborhood]) == tuple:\n",
    "            articles[sub_neighborhood].append((nlp(df.loc[i, sub_neighborhood][0]), df.loc[i, sub_neighborhood][1]))\n",
    "    print(sub_neighborhood + ' DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2014_5'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles['dorchester'][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = {'hyde_park': [], 'beacon_hill': [], 'south_boston': [], 'jamaica_plain': [], 'east_boston': [],\n",
    "                'south_end': [], 'back_bay': [], 'north_end': [], 'west_roxbury': [], 'mission_hill': [],\n",
    "                'harbor_islands': [], 'west_end': [], 'longwood_medical_area': [],\n",
    "                'dorchester': [], 'roxbury': [], 'downtown': [], 'fenway': [], 'mattapan': [], 'brighton': [],\n",
    "                'charlestown': [], 'roslindale': [], 'allston': []}\n",
    "\n",
    "for sub_neighborhood in articles.keys():\n",
    "    for (doc, article_id) in articles[sub_neighborhood]:\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == 'PERSON':\n",
    "                name = ent[0:2]\n",
    "                sentence = ent.sent\n",
    "                people[sub_neighborhood].append((name, sentence, article_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub_neighborhood in people.keys():\n",
    "    list1 = people[sub_neighborhood]\n",
    "    # insert the list to the set\n",
    "    list_set = set(list1)\n",
    "    # convert the set to the list\n",
    "    unique_list = (list(list_set))\n",
    "    people[sub_neighborhood] = unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "representation_proportions = {'hyde_park': [], 'beacon_hill': [], 'south_boston': [], 'jamaica_plain': [], 'east_boston': [],\n",
    "                'south_end': [], 'back_bay': [], 'north_end': [], 'west_roxbury': [], 'mission_hill': [],\n",
    "                'harbor_islands': [], 'west_end': [], 'longwood_medical_area': [],\n",
    "                'dorchester': [], 'roxbury': [], 'downtown': [], 'fenway': [], 'mattapan': [], 'brighton': [],\n",
    "                'charlestown': [], 'roslindale': [], 'allston': []}\n",
    "for sub_neighborhood in people.keys():\n",
    "    for i in range(len(people[sub_neighborhood])):\n",
    "        if people[sub_neighborhood][i][0].text.strip() != '':\n",
    "            temp = people[sub_neighborhood][i][0].text.split()\n",
    "            if len(temp) > 1:\n",
    "                people[sub_neighborhood][i] = (temp[-1], people[sub_neighborhood][i][1], people[sub_neighborhood][i][2])\n",
    "            else:\n",
    "                people[sub_neighborhood][i] = (temp[0], people[sub_neighborhood][i][1], people[sub_neighborhood][i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Dwyer',\n",
       " ” the report said  Dwyer is due back in court next month  and Francois will be summonsed at a later date for arraignment on prostitution and lewdness charges  officials said  A working telephone number for Francois could not be located  Joe Pesaturo  a T spokesman  said Dwyer  a 26 year veteran  is suspended with pay and “facing severe disciplinary action ”,\n",
       " '2014_300')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people['dorchester'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ethnicolr import pred_wiki_ln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp = pd.DataFrame(people[white_neighborhoods[2]], columns=['last_name', 'article', 'article_id'])\n",
    "#temp1 = pd.DataFrame(people[white_neighborhoods[3]], columns=['last_name', 'article', 'article_id'])\n",
    "#pd.concat([pred_census_ln(temp, 'last_name', 2010), pred_census_ln(temp1, 'last_name', 2010)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fenway DONE\n",
      "beacon_hill DONE\n",
      "downtown DONE\n",
      "south_boston DONE\n",
      "east_boston DONE\n",
      "back_bay DONE\n",
      "jamaica_plain DONE\n",
      "south_end DONE\n",
      "charlestown DONE\n",
      "brighton DONE\n",
      "allston DONE\n",
      "west_end DONE\n",
      "roslindale DONE\n",
      "north_end DONE\n",
      "mission_hill DONE\n",
      "harbor_islands DONE\n",
      "west_roxbury DONE\n",
      "dorchester DONE\n",
      "roxbury DONE\n",
      "mattapan DONE\n",
      "hyde_park DONE\n"
     ]
    }
   ],
   "source": [
    "final_df = pd.DataFrame(columns=['last_name', 'article', 'article_id'])\n",
    "sub_neighborhoods = white_neighborhoods + black_neighborhoods\n",
    "sub_neighborhoods.remove('longwood_medical_area')\n",
    "for col in sub_neighborhoods:\n",
    "    temp = pd.DataFrame(people[col], columns=['last_name', 'article', 'article_id'])\n",
    "    preds = pred_wiki_ln(temp, 'last_name')\n",
    "    final_df = pd.concat([final_df, preds], axis=0)\n",
    "    print(col + ' DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "agg_df = pd.DataFrame(columns=sub_neighborhoods, index=['white', 'black', 'api', 'hispanic'])\n",
    "agg_df = agg_df.fillna(0.0)\n",
    "final_df = final_df.drop(['Asian,GreaterEastAsian,EastAsian', 'Asian,GreaterEastAsian,Japanese', 'Asian,IndianSubContinent', 'GreaterAfrican,Africans', 'GreaterAfrican,Muslim', 'GreaterEuropean,British', 'GreaterEuropean,EastEuropean', 'GreaterEuropean,Jewish', 'GreaterEuropean,WestEuropean,French','GreaterEuropean,WestEuropean,Germanic','GreaterEuropean,WestEuropean,Hispanic','GreaterEuropean,WestEuropean,Italian','GreaterEuropean,WestEuropean,Nordic'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(final_df.index)):\n",
    "    if final_df.iloc[i]['race'] == 'Asian,GreaterEastAsian,EastAsian' or final_df.iloc[i]['race'] == 'Asian,GreaterEastAsian,Japanese' or final_df.iloc[i]['race'] == 'Asian,IndianSubContinent':\n",
    "        final_df.iloc[i]['race'] = 'api'\n",
    "    elif final_df.iloc[i]['race'] == 'GreaterEuropean,WestEuropean,Hispanic':\n",
    "        final_df.iloc[i]['race'] = 'hispanic'\n",
    "    elif final_df.iloc[i]['race'] == 'GreaterAfrican,Muslim' or final_df.iloc[i]['race'] == 'GreaterAfrican,Africans':\n",
    "        final_df.iloc[i]['race'] = 'black'\n",
    "    else:\n",
    "        final_df.iloc[i]['race'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['white', 'hispanic', 'api', 'black'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.race.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('People_Covered_in_the_News/people_2014.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>last_name</th>\n",
       "      <th>article</th>\n",
       "      <th>article_id</th>\n",
       "      <th>race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Coakley</td>\n",
       "      <td>(Coakley, suffers, from, comparison, to, the, ...</td>\n",
       "      <td>2014_631</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ortiz</td>\n",
       "      <td>(And, none, interfered, with, David, Ortiz,  ,...</td>\n",
       "      <td>2014_1038</td>\n",
       "      <td>hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Angela</td>\n",
       "      <td>(My, prayers, go, out, to, the, mayor, ’s, lov...</td>\n",
       "      <td>2014_1057</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Angela</td>\n",
       "      <td>(I, send, our, prayers, to, Mayor, Menino, ’s,...</td>\n",
       "      <td>2014_1057</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Barry</td>\n",
       "      <td>(but, anecdotally,  , many, of, those, infecte...</td>\n",
       "      <td>2014_778</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  last_name                                            article article_id  \\\n",
       "0   Coakley  (Coakley, suffers, from, comparison, to, the, ...   2014_631   \n",
       "1     Ortiz  (And, none, interfered, with, David, Ortiz,  ,...  2014_1038   \n",
       "2    Angela  (My, prayers, go, out, to, the, mayor, ’s, lov...  2014_1057   \n",
       "3    Angela  (I, send, our, prayers, to, Mayor, Menino, ’s,...  2014_1057   \n",
       "4     Barry  (but, anecdotally,  , many, of, those, infecte...   2014_778   \n",
       "\n",
       "       race  \n",
       "0     white  \n",
       "1  hispanic  \n",
       "2     white  \n",
       "3     white  \n",
       "4     white  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1\n",
    "# re-organize the data so that we have a way to retrieve original text\n",
    "# like adding ID to the dataset to identify each article\n",
    "# we should be able to find out the article a name comes from\n",
    "# we should also be able to find out which neighborhood an article talks about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2\n",
    "# If name has 'word', 'word', then take the first name\n",
    "# keep sentence where name occurred, okay if multiple sentences\n",
    "# look at sentence where the name was mentioned \n",
    "# and the words which were used\n",
    "# end up with a dataset which has 'name' + 'sentence' + 'race'\n",
    "# try to put ID of article in the dataset as well, next to the sentence\n",
    "# for now, try to keep the row from which the name comes, or at least some form of ID\n",
    "\n",
    "# if extra time, group sentences by associated race\n",
    "# find most frequently used words for each race, maybe a word cloud or something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3\n",
    "# come up with the population divide by races for each neighborhood\n",
    "# use neighborhood-separated articles\n",
    "# for each neighborhood, turn all articles into a spaCy-processible list of documents\n",
    "# for each list, extract out all people and run their last names with ethnicolr to predict races\n",
    "# for each set of predictions, get percentage of races\n",
    "# have journalism team go through U.S. Census data to see if the proportions of races match Census data\n",
    "\n",
    "# QUESTION: how to verify that two names talked about in an article belong to different people/the same people?\n",
    "\n",
    "# potential solution: for each article, only store the unique names; but is this possible? \n",
    "# each doc is an article, so we can extract out all \"PERSON\" entities and then keep only those which are unique\n",
    "# we could then feed the last names of those unique people (the last names may not necessarily be unique) to ethnicolr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find percentage of race represented for each neighborhood DONE\n",
    "# get unique names on an article level DONE\n",
    "\n",
    "# use sentence dataset to create a word cloud DONE\n",
    "# find all names that are black DONE\n",
    "# find all words used most commonly to talk about black people DONE\n",
    "# for every year DONE\n",
    "\n",
    "# NEXT STEPS:\n",
    "# Look through the represented race proportions for each of the functions, i.e. pred_census_ln, pred_wiki_ln, and pred_fl_reg_ln\n",
    "# Pick the most seemingly accurate result\n",
    "# Use the result to get the races of the people covered and then create the respective word clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# later goals:\n",
    "# mention of race - through names - through association - mention of \n",
    "    # neighborhoods/organizations which are predominantly one race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
