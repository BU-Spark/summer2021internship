{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import gensim\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data using Pandas\n",
    "data = pd.read_csv('globe_data/bostonglobe2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "# strip the text of unnecessary whitespaces and removing special characters\n",
    "def custom_standardization(data):\n",
    "\n",
    "    spec_chars = [\"!\",'\"',\"#\",\"%\",\"&\",\"'\",\"(\",\")\", \"*\",\"+\",\",\",\n",
    "                  \"-\",\".\",\"/\",\":\",\";\",\"<\", \"=\",\">\",\"?\",\"@\",\"[\",\n",
    "                  \"\\\\\",\"]\",\"^\",\"_\", \"`\",\"{\",\"|\",\"}\",\"~\",\"â€“\", \n",
    "                  \"\\xc2\", \"\\xa0\", \"\\x80\", \"\\x9c\", \"\\x99\", \"\\x94\", \n",
    "                  \"\\xad\", \"\\xe2\", \"\\x9d\", \"\\n\"]\n",
    "\n",
    "    for char in spec_chars:\n",
    "        data['text'] = data['text'].str.strip()\n",
    "        #data['text'] = str(data['text']).lower()\n",
    "        data['text'] = data['text'].str.replace(char, ' ')\n",
    "        #data['text'] = stemmer.stem(str(data['text']))\n",
    "        \n",
    "    return data\n",
    "\n",
    "# do stemming before training\n",
    "# compare differences between word vectors for 'black' as well as 'white' to better identify biased representations\n",
    "# think about word frequency, so that context isn't specific, but rather, general\n",
    "# increase data point size for visualizations\n",
    "\n",
    "# instead of keywords, use sub-neighborhood names\n",
    "\n",
    "# cluster similar words\n",
    "# visualize those with a meta word that's representative\n",
    "# either use average word vectors or minimizing function to get representative word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = custom_standardization(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn DataFrame into a list of lists of tokens\n",
    "documents = []\n",
    "mentions = {'hyde_park': 0, 'beacon_hill': 0, 'south_boston': 0, 'jamaica_plain': 0, 'east_boston': 0,\n",
    "           'south_end': 0, 'back_bay': 0, 'north_end': 0, 'west_roxbury': 0, 'mission_hill': 0,\n",
    "           'harbor_islands': 0, 'west_end': 0, 'south_boston_waterfront': 0, 'longwood_medical_area': 0,\n",
    "           'dorchester': 0, 'roxbury': 0, 'downtown': 0, 'fenway': 0, 'mattapan': 0, 'brighton': 0,\n",
    "           'charlestown': 0, 'roslindale': 0, 'allston': 0}\n",
    "for row in df.values:\n",
    "    [row] = row\n",
    "    temp = row.lower().split()\n",
    "    length = len(temp) - 1\n",
    "    for i in range(length):\n",
    "        if temp[i] == 'hyde' and temp[i + 1] == 'park':\n",
    "            temp[i] = 'hyde_park'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "            mentions['hyde_park'] += 1\n",
    "        elif temp[i] == 'dorchester':\n",
    "            mentions['dorchester'] += 1\n",
    "        elif temp[i] == 'roxbury':\n",
    "            mentions['roxbury'] += 1\n",
    "        elif temp[i] == 'downtown':\n",
    "            mentions['downtown'] += 1\n",
    "        elif temp[i] == 'fenway':\n",
    "            mentions['fenway'] += 1\n",
    "        elif temp[i] == 'mattapan':\n",
    "            mentions['mattapan'] += 1\n",
    "        elif temp[i] == 'brighton':\n",
    "            mentions['brighton'] += 1\n",
    "        elif temp[i] == 'charlestown':\n",
    "            mentions['charlestown'] += 1\n",
    "        elif temp[i] == 'roslindale':\n",
    "            mentions['roslindale'] += 1\n",
    "        elif temp[i] == 'allston':\n",
    "            mentions['allston'] += 1\n",
    "        elif temp[i] == 'beacon' and temp[i + 1] == 'hill':\n",
    "            temp[i] = 'beacon_hill'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "            mentions['beacon_hill'] += 1\n",
    "        elif temp[i] == 'south' and temp[i + 1] == 'boston':\n",
    "            temp[i] = 'south_boston'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "            mentions['south_boston'] += 1\n",
    "        elif temp[i] == 'jamaica' and temp[i + 1] == 'plain':\n",
    "            temp[i] = 'jamaica_plain'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "            mentions['jamaica_plain'] += 1\n",
    "        elif temp[i] == 'east' and temp[i + 1] == 'boston':\n",
    "            temp[i] = 'east_boston'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "            mentions['east_boston'] += 1\n",
    "        elif temp[i] == 'south' and temp[i + 1] == 'end':\n",
    "            temp[i] = 'south_end'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "            mentions['south_end'] += 1\n",
    "        elif temp[i] == 'back' and temp[i + 1] == 'bay':\n",
    "            temp[i] = 'back_bay'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "            mentions['back_bay'] += 1\n",
    "        elif temp[i] == 'north' and temp[i + 1] == 'end':\n",
    "            temp[i] = 'north_end'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "            mentions['north_end'] += 1\n",
    "        elif temp[i] == 'west' and temp[i + 1] == 'roxbury':\n",
    "            temp[i] = 'west_roxbury'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "            mentions['west_roxbury'] += 1\n",
    "        elif temp[i] == 'mission' and temp[i + 1] == 'hill':\n",
    "            temp[i] = 'mission_hill'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "            mentions['mission_hill'] += 1\n",
    "        elif temp[i] == 'harbor' and temp[i + 1] == 'islands':\n",
    "            temp[i] = 'harbor_islands'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "            mentions['harbor_islands'] += 1\n",
    "        elif temp[i] == 'west' and temp[i + 1] == 'end':\n",
    "            temp[i] = 'west_end'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "            mentions['west_end'] += 1\n",
    "        elif temp[i] == 'cape' and temp[i + 1] == 'verdean':\n",
    "            temp[i] = 'cape_verdean'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "        elif temp[i] == 'afro' and temp[i + 1] == 'latino':\n",
    "            temp[i] = 'afro_latino'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "        elif temp[i] == 'afro' and temp[i + 1] == 'latina':\n",
    "            temp[i] = 'afro_latina'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "        elif temp[i] == 'african' and temp[i + 1] == 'american':\n",
    "            temp[i] = 'african_american'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "        elif temp[i] == 'west' and temp[i + 1] == 'indian':\n",
    "            temp[i] = 'west_indian'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1            \n",
    "        elif i > 0 and temp[i - 1] == 'south' and temp[i] == 'boston' and temp[i + 1] == 'waterfront':\n",
    "            temp[i - 1] = 'south_boston_waterfront'\n",
    "            temp.pop(i + 1)\n",
    "            temp.pop(i)\n",
    "            length -= 2\n",
    "            mentions['south_boston_waterfront'] += 1\n",
    "        elif i > 0 and temp[i - 1] == 'longwood' and temp[i] == 'medical' and temp[i + 1] == 'area':\n",
    "            temp[i - 1] = 'longwood_medical_area'\n",
    "            temp.pop(i + 1)\n",
    "            temp.pop(i)\n",
    "            length -= 1\n",
    "            mentions['longwood_medical_area'] += 1\n",
    "        elif i >= length - 3:\n",
    "            break\n",
    "    documents.append(temp)\n",
    "    #print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Word2Vec model\n",
    "# the skip-grams method is used here, with a window of 10\n",
    "model = gensim.models.Word2Vec(window=10, min_count=2, sg=1, workers=10)\n",
    "model.build_vocab(documents)  # prepare the model vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27245590, 34085675)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model on available data\n",
    "# I use 5 epochs since that's standard\n",
    "model.train(corpus_iterable=documents, total_examples=len(documents), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_neighborhoods = ['dorchester', 'roxbury', 'mattapan', 'hyde_park']\n",
    "white_neighborhoods = ['fenway', 'beacon_hill', 'downtown', 'south_boston', 'east_boston', 'back_bay', 'jamaica_plain',\n",
    "                      'south_end', 'charlestown', 'brighton', 'allston', 'west_end', 'roslindale', 'north_end',\n",
    "                      'mission_hill', \n",
    "                       #'south_boston_waterfront', \n",
    "                       'harbor_islands', 'longwood_medical_area']\n",
    "\n",
    "keywords = black_neighborhoods + white_neighborhoods\n",
    "\n",
    "black_ethnicities = ['black', 'cape_verdean', #'afro_latino', 'afro_latina', \n",
    "                     'haitian', 'african_american', \n",
    "                     #'african-american', \n",
    "                     'caribbean', 'jamaican', 'dominican'] #, 'west_indian']\n",
    "\n",
    "# extract vector for neighborhood\n",
    "# vector for corresponding majority ethnicity\n",
    "# add those together\n",
    "# look for words that are close to that sum vector\n",
    "# repeat for other ethnicities\n",
    "# what differences do we get in terms of results for last week vs results for this week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples = []\n",
    "for sub in black_neighborhoods:\n",
    "    for ethnicity in black_ethnicities:\n",
    "        tuples.append((ethnicity, sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['original_tuple'] = 'unknown'\n",
    "\n",
    "# finding similar words and creating a csv file\n",
    "# check if similar words list includes any keywords\n",
    "# if so, remove those keywords from the similar words list\n",
    "\n",
    "def compute_similar_words(model,source_word, topn=25):\n",
    "    similar_words = [source_word]\n",
    "    try:\n",
    "        top_words = model.wv.most_similar(source_word, topn=topn)\n",
    "        length_top = len(top_words)\n",
    "        i = 0\n",
    "        while i < length_top:\n",
    "            #print('i=' + str(i) + ' len(top_words)=' + str(len(top_words)) + 'length_top=' + str(length_top))\n",
    "            if i >= length_top - 1:\n",
    "                break\n",
    "            elif top_words[i][0] in mentions.keys():\n",
    "                top_words.pop(i)\n",
    "                length_top -= 1\n",
    "            else:\n",
    "                i += 1\n",
    "        similar_words.extend([val[0] for val in top_words[:5]])\n",
    "    except KeyError as err:\n",
    "        print(err.args)\n",
    "    return similar_words    \n",
    "\n",
    "def compute_similar_words_and_representative_for_all_tasks(model, topn=25):\n",
    "    columns = ['word' + str(i - 1) for i in range(1, 7)]\n",
    "    columns.append('original_tuple')\n",
    "    df = pd.DataFrame(data=None, columns=columns)\n",
    "    reps = defaultdict()\n",
    "    for source_tuple in tuples:\n",
    "        source_word = (model.wv[source_tuple[0]] + model.wv[source_tuple[1]]) / 2\n",
    "        similar_words = compute_similar_words(model, model.wv.most_similar(positive=[source_word], topn=1), topn)\n",
    "        \n",
    "        df.loc[len(df), ['word0', 'word1', 'word2', 'word3', 'word4', 'word5']] = similar_words\n",
    "        df.loc[len(df) - 1, ['original_tuple']] = source_tuple[0] + ' ' + source_tuple[1]\n",
    "        #sum_vec = (model.wv[similar_words[0]]).copy()\n",
    "        #for i in range(1, 5):\n",
    "        #    temp = model.wv[similar_words[i]]\n",
    "        #    sum_vec += temp.copy()\n",
    "        #rep_vec = sum_vec / 5\n",
    "        #reps[source_tuple] = model.wv.most_similar(positive=[rep_vec], topn=1)\n",
    "    #for i in range(1, len(similar_words)):\n",
    "    #    sec_similar_words = compute_similar_words(model, similar_words[i], topn)\n",
    "    #    df.loc[len(df)] = sec_similar_words\n",
    "    df.to_csv('similar_words_task/ethnicity&neighborhood/2018_findings.csv')\n",
    "    #rep_ser = pd.Series(reps)\n",
    "    #rep_ser.to_csv('similar_words_task/representative_words/2018_ethnicity&neighborhoods.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:849: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n"
     ]
    }
   ],
   "source": [
    "compute_similar_words_and_representative_for_all_tasks(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nMonday  n nOpen at ownersâ€™ discretion  n nOpe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Born a decade apart  Chuck Berry and Mary Tyle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n nThe idea that bigger is better in health ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>With cyclists agitating for more safety measur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A spout of fire that burst from a gas main on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8286</th>\n",
       "      <td>n n n fair  After years of bad policy decision...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8287</th>\n",
       "      <td>The governor  the mayor  the head of the Massa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8288</th>\n",
       "      <td>The number of students disciplined in schools ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8289</th>\n",
       "      <td>Hundreds of species are facing extinction due ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8290</th>\n",
       "      <td>Boston will record its warmest December in mod...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8291 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "0     nMonday  n nOpen at ownersâ€™ discretion  n nOpe...\n",
       "1     Born a decade apart  Chuck Berry and Mary Tyle...\n",
       "2     n nThe idea that bigger is better in health ca...\n",
       "3     With cyclists agitating for more safety measur...\n",
       "4     A spout of fire that burst from a gas main on ...\n",
       "...                                                 ...\n",
       "8286  n n n fair  After years of bad policy decision...\n",
       "8287  The governor  the mayor  the head of the Massa...\n",
       "8288  The number of students disciplined in schools ...\n",
       "8289  Hundreds of species are facing extinction due ...\n",
       "8290  Boston will record its warmest December in mod...\n",
       "\n",
       "[8291 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.15635121,  0.20200919,  0.02739368,  0.16428003, -0.583524  ,\n",
       "       -0.0187033 ,  0.24306549,  0.14058374,  0.09533974, -0.05696544,\n",
       "       -0.22996135, -0.35047784, -0.3867848 ,  0.6328817 , -0.63818514,\n",
       "       -0.17376818,  0.6919726 , -0.25401035, -0.6795768 , -0.66642195,\n",
       "       -0.5047043 ,  0.18936206,  0.3376247 , -0.15336786,  0.14835308,\n",
       "       -0.09140824,  0.5957311 , -0.13208123, -0.48721436,  0.17880508,\n",
       "       -0.24239382,  0.31990367,  0.2290762 , -0.1369244 , -0.06211914,\n",
       "        0.39248827,  0.47143814,  0.17575933, -0.06704228, -0.02894345,\n",
       "       -0.03615074, -0.4128681 , -0.20304988, -0.18226594,  0.5107596 ,\n",
       "        0.31098714, -0.00776725,  0.1258231 ,  0.05229302, -0.18782486,\n",
       "       -0.0588684 , -0.22910894, -0.0870887 ,  0.2958976 ,  0.01819371,\n",
       "       -0.4072005 , -0.08461149, -0.34900177,  0.0887585 , -0.35952404,\n",
       "        0.04067158,  0.17653511,  0.38622317, -0.41997126,  0.0637475 ,\n",
       "        0.1404915 , -0.08385032, -0.6520891 , -0.82676816, -0.16945711,\n",
       "       -0.18311374,  0.07068095, -0.01744011, -0.39521623, -0.47095227,\n",
       "       -0.26899034,  0.27791265,  0.50155175,  0.41322422, -0.13444853,\n",
       "       -0.48480484, -0.04983776,  0.10086063,  0.37664732, -0.26646423,\n",
       "       -0.49172118, -0.10086288,  0.60761046,  0.14962128,  0.19990477,\n",
       "        0.22801693, -0.13757472,  0.09263148,  0.15156391,  0.28336856,\n",
       "       -0.20484091,  0.36261648, -0.356793  ,  0.02060864, -0.08033937],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[tuples[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.79906869e-01,  1.37690932e-01, -1.03308275e-01,  2.71543622e-01,\n",
       "       -3.03536326e-01,  2.45300815e-01,  5.07270336e-01, -1.25471964e-01,\n",
       "        1.82095483e-01, -7.02572688e-02,  6.58514082e-01, -1.08321421e-01,\n",
       "        7.04380795e-02,  6.67263269e-02,  6.98519051e-02, -2.79006451e-01,\n",
       "        2.01701477e-01, -3.43560159e-01, -1.69890240e-01, -5.24740338e-01,\n",
       "       -1.16725594e-01, -2.52047867e-01,  2.19401076e-01, -2.84117199e-02,\n",
       "        7.40935057e-02,  3.24531943e-01,  6.18499100e-01, -5.99122882e-01,\n",
       "       -9.92775679e-01,  1.72159731e-01,  3.29869837e-01, -5.86353242e-01,\n",
       "        2.15042919e-01, -1.26540527e-01,  2.53653824e-01, -1.13464989e-01,\n",
       "        1.39450610e-01, -1.11287639e-01, -1.67580873e-01,  1.87432274e-01,\n",
       "        1.65377796e-01, -2.55313277e-01, -1.49249107e-01,  3.25608313e-01,\n",
       "        2.93178886e-01,  2.42516756e-01,  1.67554289e-01, -4.05284166e-01,\n",
       "       -4.82737459e-02,  5.30327022e-01, -5.44127114e-02, -2.77623951e-01,\n",
       "       -4.90352809e-01,  6.75297678e-02,  7.58213103e-02, -1.95728898e-01,\n",
       "        1.59271508e-01, -5.77744469e-02,  3.19471117e-04, -3.82537007e-01,\n",
       "        1.43318534e-01,  1.67589635e-01, -9.82111916e-02,  2.88895816e-01,\n",
       "       -7.95762092e-02,  3.77531856e-01,  3.12493414e-01,  7.90202737e-01,\n",
       "       -3.61247391e-01,  1.49164602e-01,  2.94697611e-03,  1.21039614e-01,\n",
       "        2.09733192e-03, -9.75636989e-02,  5.39015457e-02,  3.00977141e-01,\n",
       "        1.48423631e-02,  2.57555336e-01, -2.28652805e-01,  3.16961616e-01,\n",
       "       -4.24597204e-01,  1.03896230e-01,  3.13423306e-01,  1.24176713e-02,\n",
       "        5.03238663e-02, -9.58404541e-02,  5.74884355e-01, -1.77628145e-01,\n",
       "        2.27971599e-02, -4.36095953e-01,  6.35092407e-02, -3.92132074e-01,\n",
       "        3.56558591e-01, -9.89276990e-02,  1.31784260e-01,  4.87926930e-01,\n",
       "        3.38641256e-01,  3.35243732e-01, -7.75871947e-02,  2.74839103e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['dorchester']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[-7.79906869e-01,  1.37690932e-01, -1.03308275e-01,  2.71543622e-01,\n",
    "       -3.03536326e-01,  2.45300815e-01,  5.07270336e-01, -1.25471964e-01,\n",
    "        1.82095483e-01, -7.02572688e-02,  6.58514082e-01, -1.08321421e-01,\n",
    "        7.04380795e-02,  6.67263269e-02,  6.98519051e-02, -2.79006451e-01,\n",
    "        2.01701477e-01, -3.43560159e-01, -1.69890240e-01, -5.24740338e-01,\n",
    "       -1.16725594e-01, -2.52047867e-01,  2.19401076e-01, -2.84117199e-02,\n",
    "        7.40935057e-02,  3.24531943e-01,  6.18499100e-01, -5.99122882e-01,\n",
    "       -9.92775679e-01,  1.72159731e-01,  3.29869837e-01, -5.86353242e-01,\n",
    "        2.15042919e-01, -1.26540527e-01,  2.53653824e-01, -1.13464989e-01,\n",
    "        1.39450610e-01, -1.11287639e-01, -1.67580873e-01,  1.87432274e-01,\n",
    "        1.65377796e-01, -2.55313277e-01, -1.49249107e-01,  3.25608313e-01,\n",
    "        2.93178886e-01,  2.42516756e-01,  1.67554289e-01, -4.05284166e-01,\n",
    "       -4.82737459e-02,  5.30327022e-01, -5.44127114e-02, -2.77623951e-01,\n",
    "       -4.90352809e-01,  6.75297678e-02,  7.58213103e-02, -1.95728898e-01,\n",
    "        1.59271508e-01, -5.77744469e-02,  3.19471117e-04, -3.82537007e-01,\n",
    "        1.43318534e-01,  1.67589635e-01, -9.82111916e-02,  2.88895816e-01,\n",
    "       -7.95762092e-02,  3.77531856e-01,  3.12493414e-01,  7.90202737e-01,\n",
    "       -3.61247391e-01,  1.49164602e-01,  2.94697611e-03,  1.21039614e-01,\n",
    "        2.09733192e-03, -9.75636989e-02,  5.39015457e-02,  3.00977141e-01,\n",
    "        1.48423631e-02,  2.57555336e-01, -2.28652805e-01,  3.16961616e-01,\n",
    "       -4.24597204e-01,  1.03896230e-01,  3.13423306e-01,  1.24176713e-02,\n",
    "        5.03238663e-02, -9.58404541e-02,  5.74884355e-01, -1.77628145e-01,\n",
    "        2.27971599e-02, -4.36095953e-01,  6.35092407e-02, -3.92132074e-01,\n",
    "        3.56558591e-01, -9.89276990e-02,  1.31784260e-01,  4.87926930e-01,\n",
    "        3.38641256e-01,  3.35243732e-01, -7.75871947e-02,  2.74839103e-01]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
