{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'south_boston_waterfront' not included\n",
    "subs = ['fenway', 'beacon_hill', 'downtown', 'south_boston', 'east_boston', 'back_bay', 'jamaica_plain',\n",
    "        'south_end', 'charlestown', 'brighton', 'allston', 'west_end', 'roslindale', 'north_end',\n",
    "        'mission_hill', 'harbor_islands', 'longwood_medical_area', 'dorchester', 'roxbury', 'mattapan', 'hyde_park']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus standardized\n",
      "\n"
     ]
    }
   ],
   "source": [
    "years = [2014, 2015, 2016, 2017, 2018]\n",
    "text_corpus = pd.DataFrame()\n",
    "for year in years:\n",
    "    temp = pd.read_csv('../../Word2Vec/globe_data/bostonglobe' + str(year) + '.csv')\n",
    "    text_corpus = pd.concat([text_corpus, temp], axis=0)\n",
    "\n",
    "def custom_standardization(data):\n",
    "\n",
    "    spec_chars = [\"!\",'\"',\"#\",\"%\",\"&\",\"'\",\"(\",\")\", \"*\",\"+\",\",\",\n",
    "                      \"-\",\".\",\"/\",\":\",\";\",\"<\", \"=\",\">\",\"?\",\"@\",\"[\",\n",
    "                      \"\\\\\",\"]\",\"^\",\"_\", \"`\",\"{\",\"|\",\"}\",\"~\",\"â€“\", \n",
    "                      \"\\xc2\", \"\\xa0\", \"\\x80\", \"\\x9c\", \"\\x99\", \"\\x94\", \n",
    "                      \"\\xad\", \"\\xe2\", \"\\x9d\", \"\\n\", \"x9d\", \"xc2\", \n",
    "                      \"xa0\", \"x80\", \"x9c\", \"x99\", \"x94\", \"xad\", \"xe2\"]\n",
    "\n",
    "    for char in spec_chars:\n",
    "        data['text'] = data['text'].str.strip()\n",
    "        #data['text'] = str(data['text']).lower()\n",
    "        data['text'] = data['text'].str.replace(char, ' ')\n",
    "        #data['text'] = stemmer.stem(str(data['text']))\n",
    "\n",
    "    return data\n",
    "\n",
    "text_corpus = custom_standardization(text_corpus)\n",
    "print('corpus standardized')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn DataFrame into a list of lists of tokens\n",
    "documents = []\n",
    "for row in text_corpus.values:\n",
    "    [row] = row\n",
    "    temp = row.lower().split()\n",
    "    documents.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagged_document(list_of_list_of_words):\n",
    "    for i, list_of_words in enumerate(list_of_list_of_words):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n",
    "data_for_training = list(tagged_document(documents))\n",
    "\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=40, min_count=2, epochs=5)\n",
    "model.build_vocab(data_for_training)\n",
    "model.train(data_for_training, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting work with fenway\n",
      "\n",
      "fenway term weights sorted\n",
      "fenway similar words to most important terms generated\n",
      "\n",
      "starting work with beacon_hill\n",
      "\n",
      "beacon_hill term weights sorted\n",
      "beacon_hill similar words to most important terms generated\n",
      "\n",
      "starting work with downtown\n",
      "\n",
      "downtown term weights sorted\n",
      "downtown similar words to most important terms generated\n",
      "\n",
      "starting work with south_boston\n",
      "\n",
      "south_boston term weights sorted\n",
      "south_boston similar words to most important terms generated\n",
      "\n",
      "starting work with east_boston\n",
      "\n",
      "east_boston term weights sorted\n",
      "east_boston similar words to most important terms generated\n",
      "\n",
      "starting work with back_bay\n",
      "\n",
      "back_bay term weights sorted\n",
      "back_bay similar words to most important terms generated\n",
      "\n",
      "starting work with jamaica_plain\n",
      "\n",
      "jamaica_plain term weights sorted\n",
      "jamaica_plain similar words to most important terms generated\n",
      "\n",
      "starting work with south_end\n",
      "\n",
      "south_end term weights sorted\n",
      "south_end similar words to most important terms generated\n",
      "\n",
      "starting work with charlestown\n",
      "\n",
      "charlestown term weights sorted\n",
      "charlestown similar words to most important terms generated\n",
      "\n",
      "starting work with brighton\n",
      "\n",
      "brighton term weights sorted\n",
      "brighton similar words to most important terms generated\n",
      "\n",
      "starting work with allston\n",
      "\n",
      "allston term weights sorted\n",
      "allston similar words to most important terms generated\n",
      "\n",
      "starting work with west_end\n",
      "\n",
      "west_end term weights sorted\n",
      "west_end similar words to most important terms generated\n",
      "\n",
      "starting work with roslindale\n",
      "\n",
      "roslindale term weights sorted\n",
      "roslindale similar words to most important terms generated\n",
      "\n",
      "starting work with north_end\n",
      "\n",
      "north_end term weights sorted\n",
      "north_end similar words to most important terms generated\n",
      "\n",
      "starting work with mission_hill\n",
      "\n",
      "mission_hill term weights sorted\n",
      "mission_hill similar words to most important terms generated\n",
      "\n",
      "starting work with harbor_islands\n",
      "\n",
      "harbor_islands term weights sorted\n",
      "harbor_islands similar words to most important terms generated\n",
      "\n",
      "starting work with longwood_medical_area\n",
      "\n",
      "longwood_medical_area term weights sorted\n",
      "longwood_medical_area similar words to most important terms generated\n",
      "\n",
      "starting work with dorchester\n",
      "\n",
      "dorchester term weights sorted\n",
      "dorchester similar words to most important terms generated\n",
      "\n",
      "starting work with roxbury\n",
      "\n",
      "roxbury term weights sorted\n",
      "roxbury similar words to most important terms generated\n",
      "\n",
      "starting work with mattapan\n",
      "\n",
      "mattapan term weights sorted\n",
      "mattapan similar words to most important terms generated\n",
      "\n",
      "starting work with hyde_park\n",
      "\n",
      "hyde_park term weights sorted\n",
      "hyde_park similar words to most important terms generated\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sub in subs:\n",
    "    print('starting work with ' + sub)\n",
    "    print()\n",
    "    sub_TFIDF = pd.DataFrame()\n",
    "    for year in years:\n",
    "        data = pd.read_csv('../../TF-IDF/Yearly_TFIDF_Scores_by_Subneighborhood/' + str(year) + '/' + 'TFIDF_' + sub + '.csv')\n",
    "        data = data.drop(['Unnamed: 0'], axis=1)\n",
    "        sub_TFIDF = pd.concat([sub_TFIDF, data], axis=0)\n",
    "\n",
    "    sub_TFIDF = sub_TFIDF.sort_values('weight', ascending=False)\n",
    "    print(sub + ' term weights sorted')\n",
    "    \n",
    "    keywords = []\n",
    "    for row in sub_TFIDF.itertuples(index=False):\n",
    "        if len(keywords) < 15 and row.term not in keywords:\n",
    "            if row.term != 'hokule':\n",
    "                keywords.append(row.term)\n",
    "                \n",
    "    # finding similar words and creating a csv file\n",
    "\n",
    "    def compute_similar_words(model,source_word, topn=5):\n",
    "        similar_words = [source_word]\n",
    "        try:\n",
    "            top_words = model.wv.most_similar(source_word, topn=topn)\n",
    "            similar_words.extend([val[0] for val in top_words])\n",
    "        except KeyError as err:\n",
    "            print(err.args)\n",
    "        return similar_words    \n",
    "\n",
    "    def compute_similar_words_for_all_tasks(model, topn=5):\n",
    "        columns = ['word' + str(i - 1) for i in range(1, topn + 2)]\n",
    "        df = pd.DataFrame(data=None, columns=columns)\n",
    "        for source_word in keywords:\n",
    "            similar_words = compute_similar_words(model, source_word, topn)\n",
    "            df.loc[len(df)] = similar_words\n",
    "        df.to_csv('similar_words_task/subneighborhood_TFIDF/' + sub + '_similar_words.csv')\n",
    "    \n",
    "    words = compute_similar_words_for_all_tasks(model)\n",
    "    print(sub + ' similar words to most important terms generated')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
