{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL Brazilian Times\n",
    "URL = 'https://www.braziliantimes.com/noticias/comunidade-brasileira'\n",
    "\n",
    "# Scrape data from page/tab 1 to below page/tab\n",
    "# At the time of writing this code, Brazilian Times had news from Jan 2020 to Jun 2021 from pages 1 to 417\n",
    "PAGES = 417"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate an empty data frame\n",
    "df = pd.DataFrame(columns=['Date', 'URL', 'Title', 'Article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "idx = 0\n",
    "for i in range(1,PAGES+1):\n",
    "    if i == 1:\n",
    "        page = requests.get(URL) # for first page\n",
    "    else:\n",
    "        page = requests.get(URL + '/' + str(i)) # for other pages except first\n",
    "    \n",
    "    # create BeautifulSoup object\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # URL's can be found under <div> with class as 'col-sm-8'\n",
    "    # We take the first two <div> which have the URL's, the last <div> does not have URL's\n",
    "    # Hence we take the first two <div>\n",
    "    for div in soup.find_all('div', {'class': 'col-sm-8'})[0:2]:\n",
    "        for a in div.find_all('a'): # URL's are under <a> inside <div>\n",
    "            df.loc[idx, 'URL'] = a.get('href') # Save URL in dataframe\n",
    "            idx += 1\n",
    "    print('page ' + str(i) + ' done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to csv with encoding = 'utf-8'\n",
    "df.to_csv('../data/BrazilianTimes_Jan2020_to_Jun2021.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv\n",
    "df = pd.read_csv('../data/BrazilianTimes_Jan2020_to_Jun2021.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Below code reads the URL, request the site, gets the date, title and article of the news \n",
    "# and saves the data back to the dataframe \n",
    "for idx in df.index:\n",
    "    url = df.loc[idx, 'URL']\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # All data related to news is under <p> with class 'date margin-bottom-10'\n",
    "    date = soup.find('p', {'class': 'date margin-bottom-10'}).text\n",
    "    date = datetime.strptime(date[13:23].strip(), '%d/%m/%Y').strftime('%m/%d/%Y') # get the date\n",
    "    title = soup.find('h1', {'class': 'title-page'}).text # get the title text\n",
    "    article = soup.find('div', {'class': 'article-body'}).text # get the article text\n",
    "    article = re.sub('[\\t\\n\\r\\s]+',' ', article).strip() # replace multiple tab, newline or whitespace with single space\n",
    "    end = article.find('Apoiem os Pequenos neg√≥cios. Mantenha a economia girando!') # ad which displays on several pages\n",
    "    if end != -1:\n",
    "        article = article[:end] # remove ad text, if present\n",
    "    df.loc[idx, ['Date', 'Title', 'Article']] = [date, title, article] # save data to dataframe\n",
    "    print('article ' + str(idx) + ' done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date column to datetime object\n",
    "df['Date'] = df['Date'].apply(lambda x: datetime.strptime(x,'%m/%d/%Y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='Date', ascending=False, inplace=True) # sort dataframe by date\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latest news of the day appears on top of every page\n",
    "# This gets captured in every page/tab\n",
    "# therefore, removing the duplicate entries\n",
    "df.drop_duplicates(subset=['URL'], keep='first', inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any rows, if they have data from 2019\n",
    "df.drop(df[df.Date.dt.year==2019].index, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to final csv\n",
    "df.to_csv('../data/BrazilianTimes_Jan2020_to_Jun2021.csv', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
