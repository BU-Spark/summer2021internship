{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit"
  },
  "interpreter": {
   "hash": "788c7a583b49e0f108948e9844ef083ea97a54690da4d8e91da56fd70fa16a57"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import gensim.models.doc2vec\n",
    "from wordcloud import WordCloud, STOPWORDS,ImageColorGenerator\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import numpy as np\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "import matplotlib.pyplot as plt \n",
    "import networkx as nx\n",
    "from random import randint \n",
    "from itertools import count\n",
    "import networkx as nx\n",
    "import csv\n",
    "from csv import reader\n",
    "import sys\n",
    "import csv\n",
    "csv.field_size_limit(256<<10)\n",
    "csv.field_size_limit()\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import gensim\n",
    "from collections import defaultdict\n",
    "#import gensim.utils.lemmatize\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data using Pandas\n",
    "data = pd.read_csv('./data-source/bostonglobe2014.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip the text of unnecessary whitespaces and removing special characters\n",
    "def custom_standardization(data):\n",
    "\n",
    "    spec_chars = [\"!\",'\"',\"#\",\"%\",\"&\",\"'\",\"(\",\")\", \"*\",\"+\",\",\",\n",
    "                  \"-\",\".\",\"/\",\":\",\";\",\"<\", \"=\",\">\",\"?\",\"@\",\"[\",\n",
    "                  \"\\\\\",\"]\",\"^\",\"_\", \"`\",\"{\",\"|\",\"}\",\"~\",\"â€“\", \n",
    "                  \"\\xc2\", \"\\xa0\", \"\\x80\", \"\\x9c\", \"\\x99\", \"\\x94\", \n",
    "                  \"\\xad\", \"\\xe2\", \"\\x9d\", \"\\n\"]\n",
    "\n",
    "    for char in spec_chars:\n",
    "        data['text'] = data['text'].str.strip()\n",
    "        data['text'] = data['text'].str.replace(char, ' ')\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-19-d04ad14f372a>:12: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n  data['text'] = data['text'].str.replace(char, ' ')\n"
     ]
    }
   ],
   "source": [
    "df = custom_standardization(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn DataFrame into a list of lists of tokens\n",
    "documents = []\n",
    "mentions = {'hyde_park': 0, 'beacon_hill': 0, 'south_boston': 0, 'jamaica_plain': 0, 'east_boston': 0,\n",
    "           'south_end': 0, 'back_bay': 0, 'north_end': 0, 'west_roxbury': 0, 'mission_hill': 0,\n",
    "           'harbor_islands': 0, 'west_end': 0, 'south_boston_waterfront': 0, 'longwood_medical_area': 0,\n",
    "           'dorchester': 0, 'roxbury': 0, 'downtown': 0, 'fenway': 0, 'mattapan': 0, 'brighton': 0,\n",
    "           'charlestown': 0, 'roslindale': 0, 'allston': 0}\n",
    "for row in df.values:\n",
    "    [row] = row\n",
    "    temp = row.lower().split()\n",
    "    length = len(temp) - 1\n",
    "    for i in range(length):\n",
    "        if temp[i] == 'hyde' and temp[i + 1] == 'park':\n",
    "            temp[i] = 'hyde_park'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "            mentions['hyde_park'] += 1\n",
    "        elif temp[i] == 'dorchester':\n",
    "            mentions['dorchester'] += 1\n",
    "        elif temp[i] == 'roxbury':\n",
    "            mentions['roxbury'] += 1\n",
    "        elif temp[i] == 'downtown':\n",
    "            mentions['downtown'] += 1\n",
    "        elif temp[i] == 'fenway':\n",
    "            mentions['fenway'] += 1\n",
    "        elif temp[i] == 'mattapan':\n",
    "            mentions['mattapan'] += 1\n",
    "        elif temp[i] == 'brighton':\n",
    "            mentions['brighton'] += 1\n",
    "        elif temp[i] == 'charlestown':\n",
    "            mentions['charlestown'] += 1\n",
    "        elif temp[i] == 'roslindale':\n",
    "            mentions['roslindale'] += 1\n",
    "        elif temp[i] == 'allston':\n",
    "            mentions['allston'] += 1\n",
    "        elif temp[i] == 'beacon' and temp[i + 1] == 'hill':\n",
    "            temp[i] = 'beacon_hill'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "            mentions['beacon_hill'] += 1\n",
    "        elif temp[i] == 'south' and temp[i + 1] == 'boston':\n",
    "            temp[i] = 'south_boston'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "            mentions['south_boston'] += 1\n",
    "        elif temp[i] == 'jamaica' and temp[i + 1] == 'plain':\n",
    "            temp[i] = 'jamaica_plain'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "            mentions['jamaica_plain'] += 1\n",
    "        elif temp[i] == 'east' and temp[i + 1] == 'boston':\n",
    "            temp[i] = 'east_boston'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "            mentions['east_boston'] += 1\n",
    "        elif temp[i] == 'south' and temp[i + 1] == 'end':\n",
    "            temp[i] = 'south_end'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "            mentions['south_end'] += 1\n",
    "        elif temp[i] == 'back' and temp[i + 1] == 'bay':\n",
    "            temp[i] = 'back_bay'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "            mentions['back_bay'] += 1\n",
    "        elif temp[i] == 'north' and temp[i + 1] == 'end':\n",
    "            temp[i] = 'north_end'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "            mentions['north_end'] += 1\n",
    "        elif temp[i] == 'west' and temp[i + 1] == 'roxbury':\n",
    "            temp[i] = 'west_roxbury'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "            mentions['west_roxbury'] += 1\n",
    "        elif temp[i] == 'mission' and temp[i + 1] == 'hill':\n",
    "            temp[i] = 'mission_hill'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "            mentions['mission_hill'] += 1\n",
    "        elif temp[i] == 'harbor' and temp[i + 1] == 'islands':\n",
    "            temp[i] = 'harbor_islands'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "            mentions['harbor_islands'] += 1\n",
    "        elif temp[i] == 'west' and temp[i + 1] == 'end':\n",
    "            temp[i] = 'west_end'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "            mentions['west_end'] += 1\n",
    "        elif i > 0 and temp[i - 1] == 'south' and temp[i] == 'boston' and temp[i + 1] == 'waterfront':\n",
    "            temp[i - 1] = 'south_boston_waterfront'\n",
    "            temp.pop(i + 1)\n",
    "            temp.pop(i)\n",
    "            length -= 2\n",
    "            mentions['south_boston_waterfront'] += 1\n",
    "        elif i > 0 and temp[i - 1] == 'longwood' and temp[i] == 'medical' and temp[i + 1] == 'area':\n",
    "            temp[i - 1] = 'longwood_medical_area'\n",
    "            temp.pop(i + 1)\n",
    "            temp.pop(i)\n",
    "            length -= 1\n",
    "            mentions['longwood_medical_area'] += 1\n",
    "        elif i >= length - 3:\n",
    "            break\n",
    "    documents.append(temp)\n",
    "    #print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Word2Vec model\n",
    "\n",
    "# convert the tokenized data into one big list of tokens, as opposed to a list of tokenized articles\n",
    "def tagged_document(list_of_list_of_words):\n",
    "   for i, list_of_words in enumerate(list_of_list_of_words):\n",
    "      yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n",
    "data_for_training = list(tagged_document(documents))\n",
    "\n",
    "# the skip-grams method is used here, with a window of 10\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=40, min_count=2, epochs=30)\n",
    "model.build_vocab(data_for_training)  # prepare the model vocabulary\n",
    "# train model on available data\n",
    "model.train(data_for_training, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words of interest\n",
    "# some words are missing in the final 'keywords' list since those do not occur in any of the articles\n",
    "\n",
    "#black_keywords = ['man', 'woman', 'men', 'women', 'male', 'female', 'person', 'people', 'community', \n",
    "#                  'neighborhood', 'child', 'children', 'kid', 'youth', 'business', 'company']\n",
    "\n",
    "#black_ethnicities = ['black', 'cape verdean', 'afro latino', 'afro latina', 'haitian', 'african american', \n",
    "#                     'african-american', 'caribbean', 'jamaican', 'dominican', 'west indian']\n",
    "\n",
    "black_neighborhoods = ['dorchester', 'roxbury', 'mattapan', 'hyde_park']\n",
    "white_neighborhoods = ['fenway', 'beacon_hill', 'downtown', 'south_boston', 'east_boston', 'back_bay', 'jamaica_plain',\n",
    "                      'south_end', 'charlestown', 'brighton', 'allston', 'west_end', 'roslindale', 'north_end',\n",
    "                      'mission_hill', \n",
    "                       #'south_boston_waterfront', \n",
    "                       'harbor_islands', 'longwood_medical_area']\n",
    "\n",
    "keywords = black_neighborhoods + white_neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = model.infer_vector(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding similar words and creating a csv file\n",
    "\n",
    "def compute_similar_words(model,source_word, topn=5):\n",
    "    similar_words = [source_word]\n",
    "    try:\n",
    "        top_words = model.wv.most_similar(source_word,topn=topn)\n",
    "        similar_words.extend([val[0] for val in top_words])\n",
    "    except KeyError as err:\n",
    "        print(err.args)\n",
    "    return similar_words    \n",
    "\n",
    "def compute_similar_words_for_all_tasks(model,topn=5):\n",
    "    columns = ['word'+str(i-1) for i in range(1,topn+2)]\n",
    "    df = pd.DataFrame(data=None,columns=columns)\n",
    "    for source_word in keywords:\n",
    "        similar_words = compute_similar_words(model,source_word,topn)\n",
    "        df.loc[len(df)] = similar_words\n",
    "    for i in range(1,len(similar_words)):\n",
    "        sec_similar_words = compute_similar_words(model,similar_words[i],topn)\n",
    "        df.loc[len(df)] = sec_similar_words\n",
    "    df.to_csv('./similar_words_task/Neighbor_50%/similar_words_task_2016.csv')\n",
    "\n",
    "words = compute_similar_words_for_all_tasks(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in similar words csv file to create WordClouds\n",
    "\n",
    "words = pd.read_csv('./similar_words_task/Neighbor_50%/similar_words_task_2016.csv')\n",
    "G = nx.Graph()\n",
    "j = 1\n",
    "for i, row in words.iterrows():\n",
    "    for j in range(1,len(row)):\n",
    "        G.add_node(i,label=row[j])\n",
    "    for j in range(1,len(row)):\n",
    "        G.add_edge(row[1], row[j])\n",
    "remove = [node for node,degree in dict(G.degree()).items() if degree > 2]\n",
    "# print(remove)\n",
    "pos = nx.spring_layout(G,k=0.3)\n",
    "betCent = nx.betweenness_centrality(G, normalized=True, endpoints=True)\n",
    "node_color = [20000.0 * G.degree(v) for v in G]\n",
    "node_size =  [v * 10000 for v in betCent.values()]\n",
    "plt.figure(figsize=(20,20))\n",
    "nx.draw_networkx(G, pos=pos, with_labels=True,\n",
    "                 node_color=node_color,\n",
    "                 node_size=node_size )\n",
    "\n",
    "model.train(data_for_training, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "plt.axis('off')\n",
    "plt.savefig('../img/Neighbor_50%/Neighbor_graph_2016.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.Series(mentions)\n",
    "freq = freq.sort_values(ascending=False)\n",
    "freq.to_csv('../model/frequency_neighbor/2016_neighbor_freq.csv')\n",
    "freq"
   ]
  }
 ]
}