{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np # linear algebra\r\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\r\n",
    "import gensim\r\n",
    "from sklearn.manifold import TSNE\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "%matplotlib inline\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "import os\r\n",
    "import random\r\n",
    "import gensim.models.doc2vec\r\n",
    "from wordcloud import WordCloud, STOPWORDS,ImageColorGenerator\r\n",
    "from gensim.models.doc2vec import Doc2Vec\r\n",
    "import numpy as np\r\n",
    "from os import path\r\n",
    "from PIL import Image\r\n",
    "from wordcloud import WordCloud, STOPWORDS \r\n",
    "import matplotlib.pyplot as plt \r\n",
    "import networkx as nx\r\n",
    "from random import randint \r\n",
    "from itertools import count\r\n",
    "import networkx as nx\r\n",
    "import ast\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "from nltk.stem import PorterStemmer\r\n",
    "from collections import Counter\r\n",
    "from num2words import num2words\r\n",
    "import openpyxl\r\n",
    "import nltk\r\n",
    "import os\r\n",
    "import string\r\n",
    "import numpy as np\r\n",
    "import copy\r\n",
    "import pandas as pd\r\n",
    "import pickle\r\n",
    "import re\r\n",
    "import math\r\n",
    "import ast\r\n",
    "\r\n",
    "import networkx as nx\r\n",
    "from random import randint \r\n",
    "from itertools import count\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = '2014'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lower_case(data):\r\n",
    "    return np.char.lower(data)\r\n",
    "\r\n",
    "def remove_stop_words(data):\r\n",
    "    stop_words = stopwords.words('english')\r\n",
    "    words = word_tokenize(str(data))\r\n",
    "    new_text = \"\"\r\n",
    "    for w in words:\r\n",
    "        if w not in stop_words and len(w) > 1:\r\n",
    "            new_text = new_text + \" \" + w\r\n",
    "    return new_text\r\n",
    "\r\n",
    "def remove_punctuation(data):\r\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\r\n",
    "    for i in range(len(symbols)):\r\n",
    "        data = np.char.replace(data, symbols[i], ' ')\r\n",
    "        data = np.char.replace(data, \"  \", \" \")\r\n",
    "    data = np.char.replace(data, ',', '')\r\n",
    "    return data\r\n",
    "\r\n",
    "def remove_apostrophe(data):\r\n",
    "    return np.char.replace(data, \"'\", \"\")\r\n",
    "\r\n",
    "def stemming(data):\r\n",
    "    stemmer= PorterStemmer()\r\n",
    "    \r\n",
    "    tokens = word_tokenize(str(data))\r\n",
    "    new_text = \"\"\r\n",
    "    for w in tokens:\r\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\r\n",
    "    return new_text\r\n",
    "\r\n",
    "def convert_numbers(data):\r\n",
    "    tokens = word_tokenize(str(data))\r\n",
    "    new_text = \"\"\r\n",
    "    for w in tokens:\r\n",
    "        try:\r\n",
    "            w = num2words(int(w))\r\n",
    "        except:\r\n",
    "            a = 0\r\n",
    "        new_text = new_text + \" \" + w\r\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\r\n",
    "    return new_text\r\n",
    "\r\n",
    "def preprocess(data):\r\n",
    "    data = convert_lower_case(data)\r\n",
    "    data = remove_punctuation(data) #remove comma seperately\r\n",
    "    data = remove_apostrophe(data)\r\n",
    "    data = remove_stop_words(data)\r\n",
    "    data = convert_numbers(data)\r\n",
    "    # data = stemming(data)\r\n",
    "    data = remove_punctuation(data)\r\n",
    "    data = convert_numbers(data)\r\n",
    "    # data = stemming(data) #needed again as we need to stem the words\r\n",
    "    data = remove_punctuation(data) #needed again as num2word is giving few hypens and commas fourty-one\r\n",
    "    data = remove_stop_words(data) #needed again as num2word is giving stop words 101 - one hundred and one\r\n",
    "    return data\r\n",
    "\r\n",
    "\r\n",
    "def compute_similar_words(model,source_word, topn=5):\r\n",
    "    similar_words = [source_word]\r\n",
    "    try:\r\n",
    "        top_words = model.wv.most_similar(source_word, topn=topn)\r\n",
    "        similar_words.extend([val[0] for val in top_words])\r\n",
    "    except KeyError as err:\r\n",
    "        print(err.args)\r\n",
    "    return similar_words    \r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Entity_Recognition/Neighborhood_Separated_Articles/'+year+'.csv')\r\n",
    "df = df.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(\"('no article', 'no_id')\")\r\n",
    "df['dorchester'] = df['dorchester'].apply(ast.literal_eval)\r\n",
    "df['roxbury'] = df['roxbury'].apply(ast.literal_eval)\r\n",
    "df['mattapan'] = df['mattapan'].apply(ast.literal_eval)\r\n",
    "df['hyde_park'] = df['hyde_park'].apply(ast.literal_eval)\r\n",
    "df['fenway'] = df['fenway'].apply(ast.literal_eval)\r\n",
    "df['beacon_hill'] = df['beacon_hill'].apply(ast.literal_eval)\r\n",
    "df['downtown'] = df['downtown'].apply(ast.literal_eval)\r\n",
    "df['south_boston'] = df['south_boston'].apply(ast.literal_eval)\r\n",
    "df['east_boston'] = df['east_boston'].apply(ast.literal_eval)\r\n",
    "df['back_bay'] = df['back_bay'].apply(ast.literal_eval)\r\n",
    "df['jamaica_plain'] = df['jamaica_plain'].apply(ast.literal_eval)\r\n",
    "df['south_end'] = df['south_end'].apply(ast.literal_eval)\r\n",
    "df['charlestown'] = df['charlestown'].apply(ast.literal_eval)\r\n",
    "df['brighton'] = df['brighton'].apply(ast.literal_eval)\r\n",
    "df['allston'] = df['allston'].apply(ast.literal_eval)\r\n",
    "df['west_end'] = df['west_end'].apply(ast.literal_eval)\r\n",
    "df['roslindale'] = df['roslindale'].apply(ast.literal_eval)\r\n",
    "df['north_end'] = df['north_end'].apply(ast.literal_eval)\r\n",
    "df['mission_hill'] = df['mission_hill'].apply(ast.literal_eval)\r\n",
    "df['harbor_islands'] = df['harbor_islands'].apply(ast.literal_eval)\r\n",
    "df['longwood_medical_area'] = df['longwood_medical_area'].apply(ast.literal_eval)\r\n",
    "df['west_roxbury'] = df['west_roxbury'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = {'hyde_park': [], 'beacon_hill': [], 'south_boston': [], 'jamaica_plain': [], 'east_boston': [],\r\n",
    "                'south_end': [], 'back_bay': [], 'north_end': [], 'west_roxbury': [], 'mission_hill': [],\r\n",
    "                'harbor_islands': [], 'west_end': [], 'longwood_medical_area': [],\r\n",
    "                'dorchester': [], 'roxbury': [], 'downtown': [], 'fenway': [], 'mattapan': [], 'brighton': [],\r\n",
    "                'charlestown': [], 'roslindale': [], 'allston': []}\r\n",
    "\r\n",
    "for col in df.columns:\r\n",
    "    tokens = []\r\n",
    "    for i in range(df.shape[0]):\r\n",
    "        article, _ = df.loc[i][col]\r\n",
    "        if article != 'no article':\r\n",
    "            text = word_tokenize(preprocess(article))\r\n",
    "            tokens = tokens + text\r\n",
    "    documents[col] = tokens\r\n",
    "    #print(col)\r\n",
    "\r\n",
    "subs = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-071643416276>:9: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  df['text'] = df['text'].str.replace(char, ' ')\n"
     ]
    }
   ],
   "source": [
    "spec_chars = [\"!\",'\"',\"#\",\"%\",\"&\",\"'\",\"(\",\")\",\r\n",
    "            \"*\",\"+\",\",\",\"-\",\".\",\"/\",\":\",\";\",\"<\",\r\n",
    "            \"=\",\">\",\"?\",\"@\",\"[\",\"\\\\\",\"]\",\"^\",\"_\",\r\n",
    "            \"`\",\"{\",\"|\",\"}\",\"~\",\"â€“\", \"\\xc2\", \"\\xa0\",\r\n",
    "            \"\\x80\", \"\\x9c\", \"\\x99\", \"\\x94\", \"\\xad\", \"\\xe2\", \"\\x9d\"]\r\n",
    "\r\n",
    "df = pd.read_csv('globe_data/bostonglobe'+year+'.csv')\r\n",
    "for char in spec_chars:\r\n",
    "    df['text'] = df['text'].str.replace(char, ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\r\n",
    "for row in df.values:\r\n",
    "    [row] = row\r\n",
    "    temp = row.lower().split()\r\n",
    "    documents.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28540168, 36608460)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(window=10, min_count=2, sg=1, workers=10)\r\n",
    "model.build_vocab(documents)  # prepare the model vocabulary\r\n",
    "#Size = 20\r\n",
    "model.train(corpus_iterable=documents, total_examples=len(documents), epochs=20)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Key 'katahdin' not present\",)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot set a row with mismatched columns",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-75283a0dd25c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msource_word\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0msimilar_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_similar_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource_word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimilar_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimilar_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0msec_similar_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_similar_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimilar_words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m         \u001b[0miloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"iloc\"\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 692\u001b[1;33m         \u001b[0miloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    693\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1628\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1629\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer_missing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1630\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1631\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer_missing\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m   1916\u001b[0m                     \u001b[1;31m# must have conforming columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1917\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cannot set a row with mismatched columns\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1920\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot set a row with mismatched columns"
     ]
    }
   ],
   "source": [
    "for col in subs:\r\n",
    "    f1 = pd.read_csv('../Doc2Vec/Model/TFIDF_NeighborMostFreq/TFIDF_' + col + '.csv', nrows=21)\r\n",
    "    keywords = f1['term'].tolist()\r\n",
    "\r\n",
    "    topn = 5\r\n",
    "    columns = ['word' + str(i - 1) for i in range(1, topn + 2)]\r\n",
    "    df = pd.DataFrame(data=None, columns=columns)\r\n",
    "    for source_word in keywords:\r\n",
    "        similar_words = compute_similar_words(model, source_word, topn)\r\n",
    "        df.loc[len(df)] = similar_words\r\n",
    "    for i in range(1, len(similar_words)):\r\n",
    "        sec_similar_words = compute_similar_words(model, similar_words[i], topn)\r\n",
    "        df.loc[len(df)] = sec_similar_words\r\n",
    "    df.to_csv('similar_words_task/TFIDF/similar_words_task_'+year+'.csv')\r\n",
    "\r\n",
    "\r\n",
    "words = pd.read_csv('similar_words_task/TFIDF/similar_words_task_'+year+'.csv')\r\n",
    "\r\n",
    "G = nx.Graph()\r\n",
    "j = 1\r\n",
    "for i, row in words.iterrows():\r\n",
    "    for j in range(1, len(row)):\r\n",
    "        G.add_node(i, label=row[j])\r\n",
    "    for j in range(1, len(row)):\r\n",
    "        G.add_edge(row[1], row[j])\r\n",
    "remove = [node for node, degree in dict(G.degree()).items() if degree > 2]\r\n",
    "\r\n",
    "pos = nx.spring_layout(G, k=0.3)\r\n",
    "betCent = nx.betweenness_centrality(G, normalized=True, endpoints=True)\r\n",
    "node_color = [20000.0 * G.degree(v) for v in G]\r\n",
    "node_size =  [v * 10000 for v in betCent.values()]\r\n",
    "plt.figure(figsize=(20, 20))\r\n",
    "nx.draw_networkx(G, pos=pos, with_labels=True,\r\n",
    "                 node_color=node_color,\r\n",
    "                 node_size=node_size)\r\n",
    "plt.axis('off')\r\n",
    "plt.savefig('word_clouds_img/TFIDF/word_cloud_'+year+'.png')\r\n",
    "plt.show()\r\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "788c7a583b49e0f108948e9844ef083ea97a54690da4d8e91da56fd70fa16a57"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
