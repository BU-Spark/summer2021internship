{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\hongx\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import gensim.models.doc2vec\n",
    "from wordcloud import WordCloud, STOPWORDS,ImageColorGenerator\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import numpy as np\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "import matplotlib.pyplot as plt \n",
    "import networkx as nx\n",
    "from random import randint \n",
    "from itertools import count\n",
    "import networkx as nx\n",
    "import csv\n",
    "from csv import reader\n",
    "import sys\n",
    "import csv\n",
    "csv.field_size_limit(256<<10)\n",
    "csv.field_size_limit()\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import gensim\n",
    "from collections import defaultdict\n",
    "#import gensim.utils.lemmatize\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-5-19bee85cbdb3>:14: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n  data['text'] = data['text'].str.replace(char, ' ')\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./data-source/bostonglobe2014.csv')\n",
    "\n",
    "# removing special characters and white space\n",
    "def custom_standardization(data):\n",
    "\n",
    "    spec_chars = [\"!\",'\"',\"#\",\"%\",\"&\",\"'\",\"(\",\")\", \"*\",\"+\",\",\",\n",
    "                  \"-\",\".\",\"/\",\":\",\";\",\"<\", \"=\",\">\",\"?\",\"@\",\"[\",\n",
    "                  \"\\\\\",\"]\",\"^\",\"_\", \"`\",\"{\",\"|\",\"}\",\"~\",\"â€“\", \n",
    "                  \"\\xc2\", \"\\xa0\", \"\\x80\", \"\\x9c\", \"\\x99\", \"\\x94\", \n",
    "                  \"\\xad\", \"\\xe2\", \"\\x9d\", \"\\n\"]\n",
    "\n",
    "    for char in spec_chars:\n",
    "        data['text'] = data['text'].str.strip()\n",
    "        data['text'] = data['text'].str.replace(char, ' ')\n",
    "        \n",
    "    return data\n",
    "\n",
    "df = custom_standardization(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn DataFrame into a list of lists of tokens\n",
    "documents = []\n",
    "mentions = {'hyde_park': 0, 'beacon_hill': 0, 'south_boston': 0, 'jamaica_plain': 0, 'east_boston': 0,\n",
    "           'south_end': 0, 'back_bay': 0, 'north_end': 0, 'west_roxbury': 0, 'mission_hill': 0,\n",
    "           'harbor_islands': 0, 'west_end': 0, 'south_boston_waterfront': 0, 'longwood_medical_area': 0,\n",
    "           'dorchester': 0, 'roxbury': 0, 'downtown': 0, 'fenway': 0, 'mattapan': 0, 'brighton': 0,\n",
    "           'charlestown': 0, 'roslindale': 0, 'allston': 0}\n",
    "for row in df.values:\n",
    "    [row] = row\n",
    "    temp = row.lower().split()\n",
    "    length = len(temp) - 1\n",
    "    for i in range(length):\n",
    "        if temp[i] == 'hyde' and temp[i + 1] == 'park':\n",
    "            temp[i] = 'hyde_park'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "            mentions['hyde_park'] += 1\n",
    "        elif temp[i] == 'dorchester':\n",
    "            mentions['dorchester'] += 1\n",
    "        elif temp[i] == 'roxbury':\n",
    "            mentions['roxbury'] += 1\n",
    "        elif temp[i] == 'downtown':\n",
    "            mentions['downtown'] += 1\n",
    "        elif temp[i] == 'fenway':\n",
    "            mentions['fenway'] += 1\n",
    "        elif temp[i] == 'mattapan':\n",
    "            mentions['mattapan'] += 1\n",
    "        elif temp[i] == 'brighton':\n",
    "            mentions['brighton'] += 1\n",
    "        elif temp[i] == 'charlestown':\n",
    "            mentions['charlestown'] += 1\n",
    "        elif temp[i] == 'roslindale':\n",
    "            mentions['roslindale'] += 1\n",
    "        elif temp[i] == 'allston':\n",
    "            mentions['allston'] += 1\n",
    "        elif temp[i] == 'beacon' and temp[i + 1] == 'hill':\n",
    "            temp[i] = 'beacon_hill'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "            mentions['beacon_hill'] += 1\n",
    "        elif temp[i] == 'south' and temp[i + 1] == 'boston':\n",
    "            temp[i] = 'south_boston'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "            mentions['south_boston'] += 1\n",
    "        elif temp[i] == 'jamaica' and temp[i + 1] == 'plain':\n",
    "            temp[i] = 'jamaica_plain'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "            mentions['jamaica_plain'] += 1\n",
    "        elif temp[i] == 'east' and temp[i + 1] == 'boston':\n",
    "            temp[i] = 'east_boston'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "            mentions['east_boston'] += 1\n",
    "        elif temp[i] == 'south' and temp[i + 1] == 'end':\n",
    "            temp[i] = 'south_end'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "            mentions['south_end'] += 1\n",
    "        elif temp[i] == 'back' and temp[i + 1] == 'bay':\n",
    "            temp[i] = 'back_bay'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "            mentions['back_bay'] += 1\n",
    "        elif temp[i] == 'north' and temp[i + 1] == 'end':\n",
    "            temp[i] = 'north_end'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "            mentions['north_end'] += 1\n",
    "        elif temp[i] == 'west' and temp[i + 1] == 'roxbury':\n",
    "            temp[i] = 'west_roxbury'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "            mentions['west_roxbury'] += 1\n",
    "        elif temp[i] == 'mission' and temp[i + 1] == 'hill':\n",
    "            temp[i] = 'mission_hill'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "            mentions['mission_hill'] += 1\n",
    "        elif temp[i] == 'harbor' and temp[i + 1] == 'islands':\n",
    "            temp[i] = 'harbor_islands'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "            mentions['harbor_islands'] += 1\n",
    "        elif temp[i] == 'west' and temp[i + 1] == 'end':\n",
    "            temp[i] = 'west_end'\n",
    "            temp.pop(i + 1)\n",
    "            length -= 1\n",
    "            mentions['west_end'] += 1\n",
    "        elif i > 0 and temp[i - 1] == 'south' and temp[i] == 'boston' and temp[i + 1] == 'waterfront':\n",
    "            temp[i - 1] = 'south_boston_waterfront'\n",
    "            temp.pop(i + 1)\n",
    "            temp.pop(i)\n",
    "            length -= 2\n",
    "            mentions['south_boston_waterfront'] += 1\n",
    "        elif i > 0 and temp[i - 1] == 'longwood' and temp[i] == 'medical' and temp[i + 1] == 'area':\n",
    "            temp[i - 1] = 'longwood_medical_area'\n",
    "            temp.pop(i + 1)\n",
    "            temp.pop(i)\n",
    "            length -= 1\n",
    "            mentions['longwood_medical_area'] += 1\n",
    "        elif i >= length - 3:\n",
    "            break\n",
    "    documents.append(temp)\n",
    "    #print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Black_keywords = ['man', 'woman', 'men', 'women', 'male', 'female', 'person', 'people', 'community', 'neighborhood', 'child', 'children', 'kid', 'youth', 'business', 'company']\n",
    "\n",
    "Black_ethnicities = ['black', 'cape verdean', 'afro latino', 'afro latina', 'haitian', 'african american', 'african-american', 'caribbean', 'jamaican', 'dominican', 'west indian']\n",
    "\n",
    "#Neighborhood with 50% or more Black\n",
    "Neighbor_Black = ['Mattapan','Dorchester','Mattapan','Dorchester','Roxbury','Hyde_Park', 'Roslindale', 'Roxbury','West Roxbury']\n",
    "\n",
    "white_neighborhoods = ['fenway', 'beacon_hill', 'downtown', 'south_boston', 'east_boston', 'back_bay', 'jamaica_plain', 'south_end', 'charlestown', 'brighton', 'allston', 'west_end', 'roslindale', 'north_end', 'mission_hill', 'harbor_islands', 'longwood_medical_area']\n",
    "#'south_boston_waterfront', \n",
    "\n",
    "\n",
    "\n",
    "#pick words from above \n",
    "\n",
    "keywords = ['mattapan','dorchester','roxbury','roslindale', 'roxbury','hyde_park','west_roxbury']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up doc2vec model=================================================\n",
    "\n",
    "\n",
    "# tokenize the data and clean it using the simple_preprocess method\n",
    "def tokenize(text, stopwords, max_len = 20):\n",
    "    return [token for token in gensim.utils.simple_preprocess(text, max_len=max_len) if token not in stopwords]\n",
    "\n",
    "# convert the tokenized data into one big list of tokens, as opposed to a list of tokenized articles\n",
    "def target_doc (df):\n",
    "    articles = df.values.tolist()\n",
    "    articles_flat = [item for sublist in articles for item in sublist]\n",
    "    tagged_docs = [gensim.models.doc2vec.TaggedDocument(tokenize(text, [], max_len=200), [i]) for i, text in enumerate(articles_flat)]\n",
    "    return tagged_docs\n",
    "\n",
    "# create the Doc2Vec model, build the model vocabulary, and train the model on the data\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=30, epochs=40, window=2, dm=1)\n",
    "model.build_vocab(target_doc(df))\n",
    "model.train(target_doc(df), total_examples=model.corpus_count, epochs=model.epochs)\n",
    "vector = model.infer_vector(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(\"Key 'west_roxbury' not present\",)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "cannot set a row with mismatched columns",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-3b7604d01f05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;31m# df.to_csv('./similar_words_task/similar_words_task_2018.csv')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_similar_words_for_all_tasks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-51-3b7604d01f05>\u001b[0m in \u001b[0;36mcompute_similar_words_for_all_tasks\u001b[1;34m(model, topn)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msource_word\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0msimilar_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_similar_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msource_word\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtopn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimilar_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimilar_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0msec_similar_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_similar_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msimilar_words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtopn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m         \u001b[0miloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"iloc\"\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 692\u001b[1;33m         \u001b[0miloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    693\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1628\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1629\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer_missing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1630\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1631\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer_missing\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m   1916\u001b[0m                     \u001b[1;31m# must have conforming columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1917\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cannot set a row with mismatched columns\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1920\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot set a row with mismatched columns"
     ]
    }
   ],
   "source": [
    "#finding similar words and creating a csv file\n",
    "\n",
    "#def compute_similar_words(model,source_word, topn=5):\n",
    "#    similar_words = [source_word]\n",
    "#    try:\n",
    "#        top_words = model.wv.most_similar(source_word,topn=topn)\n",
    "#        similar_words.extend([val[0] for val in top_words])\n",
    "#    except KeyError as err:\n",
    "#        print(err.args)\n",
    "#    return similar_words\n",
    "\n",
    "def compute_similar_words(model,source_word, topn=25):\n",
    "    similar_words = [source_word]\n",
    "    try:\n",
    "        top_words = model.wv.most_similar(source_word, topn=topn)\n",
    "        length_top = len(top_words)\n",
    "        i = 0\n",
    "        while i < length_top:\n",
    "            #print('i=' + str(i) + ' len(top_words)=' + str(len(top_words)) + 'length_top=' + str(length_top))\n",
    "            if i >= length_top - 1:\n",
    "                break\n",
    "            elif top_words[i][0] in mentions.keys():\n",
    "                top_words.pop(i)\n",
    "                length_top -= 1\n",
    "            else:\n",
    "                i += 1\n",
    "        similar_words.extend([val[0] for val in top_words[:5]])\n",
    "    except KeyError as err:\n",
    "        print(err.args)\n",
    "    return similar_words    \n",
    "\n",
    "def compute_similar_words_for_all_tasks(model,topn=25):\n",
    "    columns = ['word'+str(i-1) for i in range(1,5+2)]\n",
    "    df = pd.DataFrame(data=None,columns=columns)\n",
    "    for source_word in keywords:\n",
    "        similar_words = compute_similar_words(model,source_word,topn)\n",
    "        df.loc[len(df)] = similar_words\n",
    "    for i in range(1,len(similar_words)):\n",
    "        sec_similar_words = compute_similar_words(model,similar_words[i],topn)\n",
    "        df.loc[len(df)] = sec_similar_words\n",
    "    df.to_csv('./similar_words_task/Neighbor_50%/similar_words_task_2014.csv')\n",
    "    # df.to_csv('./similar_words_task/similar_words_task_2015.csv')\n",
    "    # df.to_csv('./similar_words_task/similar_words_task_2016.csv')\n",
    "    # df.to_csv('./similar_words_task/similar_words_task_2017.csv')\n",
    "    # df.to_csv('./similar_words_task/similar_words_task_2018.csv')\n",
    "\n",
    "words = compute_similar_words_for_all_tasks(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in similar words csv file to create WordClouds\n",
    "\n",
    "words = pd.read_csv('./similar_words_task/Neighbor_50%/similar_words_task_2014.csv')\n",
    "# words = pd.read_csv('./similar_words_task/similar_words_task_2015.csv')\n",
    "# words = pd.read_csv('./similar_words_task/similar_words_task_2016.csv')\n",
    "# words = pd.read_csv('./similar_words_task/similar_words_task_2017.csv')\n",
    "# words = pd.read_csv('./similar_words_task/similar_words_task_2018.csv')\n",
    "\n",
    "G = nx.Graph()\n",
    "j = 1\n",
    "for i, row in words.iterrows():\n",
    "    for j in range(1,len(row)):\n",
    "        G.add_node(i,label=row[j])\n",
    "    for j in range(1,len(row)):\n",
    "        G.add_edge(row[1], row[j])\n",
    "remove = [node for node,degree in dict(G.degree()).items() if degree > 2]\n",
    "# print(remove)\n",
    "pos = nx.spring_layout(G,k=0.3)\n",
    "betCent = nx.betweenness_centrality(G, normalized=True, endpoints=True)\n",
    "node_color = [20000.0 * G.degree(v) for v in G]\n",
    "node_size =  [v * 10000 for v in betCent.values()]\n",
    "plt.figure(figsize=(20,20))\n",
    "nx.draw_networkx(G, pos=pos, with_labels=True,\n",
    "                 node_color=node_color,\n",
    "                 node_size=node_size )\n",
    "plt.axis('off')\n",
    "plt.savefig(\"../img/Neighbor_50%)Neighbor_graph_2014.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "788c7a583b49e0f108948e9844ef083ea97a54690da4d8e91da56fd70fa16a57"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}