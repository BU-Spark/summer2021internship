{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have (article text, article id) tuples for each neighborhood\n",
    "# turn all articles into spacy-processible list of documents, but maintain the tuples\n",
    "    # the result would be a tuple of a list of documents and the article id\n",
    "# for each list, extract out all people, feed their last names to ethnicolr to predict races and then tag the result with the article id\n",
    "# for each neighborhood, get percentage of races\n",
    "# journalism team will compare results with U.S. Census data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import en_core_web_md\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load medium English model in case we need to work with vectors\n",
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Neighborhood_Separated_Articles/2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_neighborhoods = ['dorchester', 'roxbury', 'mattapan', 'hyde_park']\n",
    "white_neighborhoods = ['fenway', 'beacon_hill', 'downtown', 'south_boston', 'east_boston', 'back_bay', 'jamaica_plain',\n",
    "                      'south_end', 'charlestown', 'brighton', 'allston', 'west_end', 'roslindale', 'north_end',\n",
    "                      'mission_hill', 'harbor_islands', 'longwood_medical_area', 'west_roxbury']\n",
    "df = df.fillna(\"('no article', 'no_id')\")\n",
    "df['dorchester'] = df['dorchester'].apply(ast.literal_eval)\n",
    "df['roxbury'] = df['roxbury'].apply(ast.literal_eval)\n",
    "df['mattapan'] = df['mattapan'].apply(ast.literal_eval)\n",
    "df['hyde_park'] = df['hyde_park'].apply(ast.literal_eval)\n",
    "df['fenway'] = df['fenway'].apply(ast.literal_eval)\n",
    "df['beacon_hill'] = df['beacon_hill'].apply(ast.literal_eval)\n",
    "df['downtown'] = df['downtown'].apply(ast.literal_eval)\n",
    "df['south_boston'] = df['south_boston'].apply(ast.literal_eval)\n",
    "df['east_boston'] = df['east_boston'].apply(ast.literal_eval)\n",
    "df['back_bay'] = df['back_bay'].apply(ast.literal_eval)\n",
    "df['jamaica_plain'] = df['jamaica_plain'].apply(ast.literal_eval)\n",
    "df['south_end'] = df['south_end'].apply(ast.literal_eval)\n",
    "df['charlestown'] = df['charlestown'].apply(ast.literal_eval)\n",
    "df['brighton'] = df['brighton'].apply(ast.literal_eval)\n",
    "df['allston'] = df['allston'].apply(ast.literal_eval)\n",
    "df['west_end'] = df['west_end'].apply(ast.literal_eval)\n",
    "df['roslindale'] = df['roslindale'].apply(ast.literal_eval)\n",
    "df['north_end'] = df['north_end'].apply(ast.literal_eval)\n",
    "df['mission_hill'] = df['mission_hill'].apply(ast.literal_eval)\n",
    "df['harbor_islands'] = df['harbor_islands'].apply(ast.literal_eval)\n",
    "df['longwood_medical_area'] = df['longwood_medical_area'].apply(ast.literal_eval)\n",
    "df['west_roxbury'] = df['west_roxbury'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_chars = [\"!\",'\"',\"#\",\"%\",\"&\",\"'\",\"(\",\")\", \"*\",\"+\",\",\",\n",
    "                  \"-\",\".\",\"/\",\":\",\";\",\"<\", \"=\",\">\",\"?\",\"@\",\"[\",\n",
    "                  \"\\\\\",\"]\",\"^\",\"_\", \"`\",\"{\",\"|\",\"}\",\"~\",\"–\", \n",
    "                  \"\\xc2\", \"\\xa0\", \"\\x80\", \"\\x9c\", \"\\x99\", \"\\x94\", \n",
    "                  \"\\xad\", \"\\xe2\", \"\\x9d\", \"\\n\"]\n",
    "\n",
    "df = df.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "#for char in spec_chars:\n",
    "#    df['text'] = df['text'].str.strip()\n",
    "#    df['text'] = df['text'].str.replace(char, ' ')\n",
    "       \n",
    "# access each column separately\n",
    "for i in range(len(df.index)):\n",
    "    for col in df.columns:\n",
    "        for char in spec_chars:\n",
    "            try:\n",
    "                df.loc[i, col][0] = df.loc[i, col][0].str.strip()\n",
    "                df.loc[i, col][0] = df.loc[i, col][0].str.replace(char, ' ')\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1783, 22)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyde_park DONE\n",
      "beacon_hill DONE\n",
      "south_boston DONE\n",
      "jamaica_plain DONE\n",
      "east_boston DONE\n",
      "south_end DONE\n",
      "back_bay DONE\n",
      "north_end DONE\n",
      "west_roxbury DONE\n",
      "mission_hill DONE\n",
      "harbor_islands DONE\n",
      "west_end DONE\n",
      "longwood_medical_area DONE\n",
      "dorchester DONE\n",
      "roxbury DONE\n",
      "downtown DONE\n",
      "fenway DONE\n",
      "mattapan DONE\n",
      "brighton DONE\n",
      "charlestown DONE\n",
      "roslindale DONE\n",
      "allston DONE\n"
     ]
    }
   ],
   "source": [
    "articles = {'hyde_park': [], 'beacon_hill': [], 'south_boston': [], 'jamaica_plain': [], 'east_boston': [],\n",
    "                'south_end': [], 'back_bay': [], 'north_end': [], 'west_roxbury': [], 'mission_hill': [],\n",
    "                'harbor_islands': [], 'west_end': [], 'longwood_medical_area': [],\n",
    "                'dorchester': [], 'roxbury': [], 'downtown': [], 'fenway': [], 'mattapan': [], 'brighton': [],\n",
    "                'charlestown': [], 'roslindale': [], 'allston': []}\n",
    "for sub_neighborhood in articles.keys():\n",
    "    for i in range(df.shape[0]):\n",
    "        if type(df.loc[i, sub_neighborhood]) == tuple:\n",
    "            articles[sub_neighborhood].append((nlp(df.loc[i, sub_neighborhood][0]), df.loc[i, sub_neighborhood][1]))\n",
    "    print(sub_neighborhood + ' DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018_3'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles['dorchester'][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = {'hyde_park': [], 'beacon_hill': [], 'south_boston': [], 'jamaica_plain': [], 'east_boston': [],\n",
    "                'south_end': [], 'back_bay': [], 'north_end': [], 'west_roxbury': [], 'mission_hill': [],\n",
    "                'harbor_islands': [], 'west_end': [], 'longwood_medical_area': [],\n",
    "                'dorchester': [], 'roxbury': [], 'downtown': [], 'fenway': [], 'mattapan': [], 'brighton': [],\n",
    "                'charlestown': [], 'roslindale': [], 'allston': []}\n",
    "\n",
    "for sub_neighborhood in articles.keys():\n",
    "    for (doc, article_id) in articles[sub_neighborhood]:\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == 'PERSON':\n",
    "                name = ent[0:2]\n",
    "                sentence = ent.sent\n",
    "                people[sub_neighborhood].append((name, sentence, article_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub_neighborhood in people.keys():\n",
    "    list1 = people[sub_neighborhood]\n",
    "    # insert the list to the set\n",
    "    list_set = set(list1)\n",
    "    # convert the set to the list\n",
    "    unique_list = (list(list_set))\n",
    "    people[sub_neighborhood] = unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "representation_proportions = {'hyde_park': [], 'beacon_hill': [], 'south_boston': [], 'jamaica_plain': [], 'east_boston': [],\n",
    "                'south_end': [], 'back_bay': [], 'north_end': [], 'west_roxbury': [], 'mission_hill': [],\n",
    "                'harbor_islands': [], 'west_end': [], 'longwood_medical_area': [],\n",
    "                'dorchester': [], 'roxbury': [], 'downtown': [], 'fenway': [], 'mattapan': [], 'brighton': [],\n",
    "                'charlestown': [], 'roslindale': [], 'allston': []}\n",
    "for sub_neighborhood in people.keys():\n",
    "    for i in range(len(people[sub_neighborhood])):\n",
    "        if people[sub_neighborhood][i][0].text.strip() != '':\n",
    "            temp = people[sub_neighborhood][i][0].text.split()\n",
    "            if len(temp) > 1:\n",
    "                people[sub_neighborhood][i] = (temp[-1], people[sub_neighborhood][i][1], people[sub_neighborhood][i][2])\n",
    "            else:\n",
    "                people[sub_neighborhood][i] = (temp[0], people[sub_neighborhood][i][1], people[sub_neighborhood][i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Melvin',\n",
       " Dr  Jim O’Connell  who leads Boston Health Care for the Homeless  was among those looking for Melvin  ,\n",
       " '2018_2901')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people['dorchester'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\ASUS\\anaconda3\\envs\\dcamm_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ASUS\\anaconda3\\envs\\dcamm_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ASUS\\anaconda3\\envs\\dcamm_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ASUS\\anaconda3\\envs\\dcamm_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ASUS\\anaconda3\\envs\\dcamm_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ASUS\\anaconda3\\envs\\dcamm_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from ethnicolr import census_ln, pred_census_ln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp = pd.DataFrame(people[white_neighborhoods[2]], columns=['last_name', 'article', 'article_id'])\n",
    "#temp1 = pd.DataFrame(people[white_neighborhoods[3]], columns=['last_name', 'article', 'article_id'])\n",
    "#pd.concat([pred_census_ln(temp, 'last_name', 2010), pred_census_ln(temp1, 'last_name', 2010)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ASUS\\anaconda3\\envs\\dcamm_env\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\ASUS\\anaconda3\\envs\\dcamm_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1190: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\ASUS\\anaconda3\\envs\\dcamm_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2880: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\ASUS\\anaconda3\\envs\\dcamm_env\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "fenway DONE\n",
      "beacon_hill DONE\n",
      "downtown DONE\n",
      "south_boston DONE\n",
      "east_boston DONE\n",
      "back_bay DONE\n",
      "jamaica_plain DONE\n",
      "south_end DONE\n",
      "charlestown DONE\n",
      "brighton DONE\n",
      "allston DONE\n",
      "west_end DONE\n",
      "roslindale DONE\n",
      "north_end DONE\n",
      "mission_hill DONE\n",
      "harbor_islands DONE\n",
      "west_roxbury DONE\n",
      "dorchester DONE\n",
      "roxbury DONE\n",
      "mattapan DONE\n",
      "hyde_park DONE\n"
     ]
    }
   ],
   "source": [
    "final_df = pd.DataFrame(columns=['last_name', 'article', 'article_id'])\n",
    "sub_neighborhoods = white_neighborhoods + black_neighborhoods\n",
    "sub_neighborhoods.remove('longwood_medical_area')#\n",
    "for col in sub_neighborhoods:\n",
    "    temp = pd.DataFrame(people[col], columns=['last_name', 'article', 'article_id'])\n",
    "    preds = pred_census_ln(temp, 'last_name', 2010)\n",
    "    final_df = pd.concat([final_df, preds], axis=0)\n",
    "    print(col + ' DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('People_Covered_the_News/people_2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1\n",
    "# re-organize the data so that we have a way to retrieve original text\n",
    "# like adding ID to the dataset to identify each article\n",
    "# we should be able to find out the article a name comes from\n",
    "# we should also be able to find out which neighborhood an article talks about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2\n",
    "# If name has 'word', 'word', then take the first name\n",
    "# keep sentence where name occurred, okay if multiple sentences\n",
    "# look at sentence where the name was mentioned \n",
    "# and the words which were used\n",
    "# end up with a dataset which has 'name' + 'sentence' + 'race'\n",
    "# try to put ID of article in the dataset as well, next to the sentence\n",
    "# for now, try to keep the row from which the name comes, or at least some form of ID\n",
    "\n",
    "# if extra time, group sentences by associated race\n",
    "# find most frequently used words for each race, maybe a word cloud or something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3\n",
    "# come up with the population divide by races for each neighborhood\n",
    "# use neighborhood-separated articles\n",
    "# for each neighborhood, turn all articles into a spaCy-processible list of documents\n",
    "# for each list, extract out all people and run their last names with ethnicolr to predict races\n",
    "# for each set of predictions, get percentage of races\n",
    "# have journalism team go through U.S. Census data to see if the proportions of races match Census data\n",
    "\n",
    "# QUESTION: how to verify that two names talked about in an article belong to different people/the same people?\n",
    "\n",
    "# potential solution: for each article, only store the unique names; but is this possible? \n",
    "# each doc is an article, so we can extract out all \"PERSON\" entities and then keep only those which are unique\n",
    "# we could then feed the last names of those unique people (the last names may not necessarily be unique) to ethnicolr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>last_name</th>\n",
       "      <th>article</th>\n",
       "      <th>article_id</th>\n",
       "      <th>race</th>\n",
       "      <th>api</th>\n",
       "      <th>black</th>\n",
       "      <th>hispanic</th>\n",
       "      <th>white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pedro</td>\n",
       "      <td>(A, torrent, of, new, multiethnic, fans, spurr...</td>\n",
       "      <td>2018_4390</td>\n",
       "      <td>hispanic</td>\n",
       "      <td>0.023359</td>\n",
       "      <td>0.023895</td>\n",
       "      <td>0.636928</td>\n",
       "      <td>0.327882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Faneuil</td>\n",
       "      <td>(most, famous, edifices, and, a, popular, tour...</td>\n",
       "      <td>2018_797</td>\n",
       "      <td>white</td>\n",
       "      <td>0.032160</td>\n",
       "      <td>0.086144</td>\n",
       "      <td>0.098934</td>\n",
       "      <td>0.481488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Phillips</td>\n",
       "      <td>(Boston, based, composer, Tom, Phillips, who, ...</td>\n",
       "      <td>2018_4213</td>\n",
       "      <td>white</td>\n",
       "      <td>0.011399</td>\n",
       "      <td>0.249012</td>\n",
       "      <td>0.031273</td>\n",
       "      <td>0.988014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yawkey</td>\n",
       "      <td>(It, was, a, proposal, that, had, for, weeks, ...</td>\n",
       "      <td>2018_1101</td>\n",
       "      <td>white</td>\n",
       "      <td>0.009230</td>\n",
       "      <td>0.134884</td>\n",
       "      <td>0.025013</td>\n",
       "      <td>0.981613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kraft</td>\n",
       "      <td>(mom, could, not, attend, the, Ring, Party, at...</td>\n",
       "      <td>2018_2122</td>\n",
       "      <td>white</td>\n",
       "      <td>0.004162</td>\n",
       "      <td>0.003855</td>\n",
       "      <td>0.007275</td>\n",
       "      <td>0.848404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  last_name                                            article article_id  \\\n",
       "0     Pedro  (A, torrent, of, new, multiethnic, fans, spurr...  2018_4390   \n",
       "1   Faneuil  (most, famous, edifices, and, a, popular, tour...   2018_797   \n",
       "2  Phillips  (Boston, based, composer, Tom, Phillips, who, ...  2018_4213   \n",
       "3    Yawkey  (It, was, a, proposal, that, had, for, weeks, ...  2018_1101   \n",
       "4     Kraft  (mom, could, not, attend, the, Ring, Party, at...  2018_2122   \n",
       "\n",
       "       race       api     black  hispanic     white  \n",
       "0  hispanic  0.023359  0.023895  0.636928  0.327882  \n",
       "1     white  0.032160  0.086144  0.098934  0.481488  \n",
       "2     white  0.011399  0.249012  0.031273  0.988014  \n",
       "3     white  0.009230  0.134884  0.025013  0.981613  \n",
       "4     white  0.004162  0.003855  0.007275  0.848404  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find percentage of race represented for each neighborhood\n",
    "# get unique names on an article level\n",
    "\n",
    "# use sentence dataset to create a word cloud\n",
    "# find all names that are black\n",
    "# find all words used most commonly to talk about black people\n",
    "# for every year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# later goals:\n",
    "# mention of race - through names - through association - mention of \n",
    "    # neighborhoods/organizations which are predominantly one race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
