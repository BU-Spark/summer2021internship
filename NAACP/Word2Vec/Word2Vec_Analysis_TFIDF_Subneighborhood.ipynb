{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc5f9fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import gensim\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from num2words import num2words\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import math\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b688864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "def convert_lower_case(data):\n",
    "    return np.char.lower(str(data))\n",
    "\n",
    "def remove_stop_words(data):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text\n",
    "\n",
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data\n",
    "\n",
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")\n",
    "\n",
    "def stemming(data):\n",
    "    stemmer= PorterStemmer()\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text\n",
    "\n",
    "def convert_numbers(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return new_text\n",
    "\n",
    "def preprocess(data):\n",
    "    data = convert_lower_case(data)\n",
    "    data = remove_punctuation(data) #remove comma seperately\n",
    "    data = remove_apostrophe(data)\n",
    "    data = remove_stop_words(data)\n",
    "    data = convert_numbers(data)\n",
    "    #data = stemming(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = convert_numbers(data)\n",
    "    #data = stemming(data) #needed again as we need to stem the words\n",
    "    data = remove_punctuation(data) #needed again as num2word is giving few hypens and commas fourty-one\n",
    "    data = remove_stop_words(data) #needed again as num2word is giving stop words 101 - one hundred and one\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7345933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus standardized\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7129399, 9147370)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years = [2014]\n",
    "text_corpus = pd.DataFrame()\n",
    "for year in years:\n",
    "    temp = pd.read_csv('globe_data/bostonglobe' + str(year) + '.csv')\n",
    "    text_corpus = pd.concat([text_corpus, temp], axis=0)\n",
    "\n",
    "def custom_standardization(data):\n",
    "\n",
    "    spec_chars = [\"!\",'\"',\"#\",\"%\",\"&\",\"'\",\"(\",\")\", \"*\",\"+\",\",\",\n",
    "                      \"-\",\".\",\"/\",\":\",\";\",\"<\", \"=\",\">\",\"?\",\"@\",\"[\",\n",
    "                      \"\\\\\",\"]\",\"^\",\"_\", \"`\",\"{\",\"|\",\"}\",\"~\",\"â€“\", \n",
    "                      \"\\xc2\", \"\\xa0\", \"\\x80\", \"\\x9c\", \"\\x99\", \"\\x94\", \n",
    "                      \"\\xad\", \"\\xe2\", \"\\x9d\", \"\\n\", \"x9d\", \"xc2\", \n",
    "                      \"xa0\", \"x80\", \"x9c\", \"x99\", \"x94\", \"xad\", \"xe2\"]\n",
    "\n",
    "    for char in spec_chars:\n",
    "        # did not stem/lemmatize since doing so gave weird errors \n",
    "        # words found in an article didn't exist in the corpus for that year\n",
    "        data['text'] = data['text'].str.strip()\n",
    "        data['text'] = data['text'].str.replace(char, ' ')\n",
    "        data['text'] = data['text'].str.lower()\n",
    "\n",
    "    return data\n",
    "\n",
    "text_corpus = custom_standardization(text_corpus)\n",
    "print('corpus standardized')\n",
    "print()\n",
    "    \n",
    "# turn DataFrame into a list of lists of tokens\n",
    "documents = []\n",
    "for row in text_corpus.values:\n",
    "    [row] = row\n",
    "    temp = row.lower().split()\n",
    "    documents.append(temp)\n",
    "\n",
    "# create Word2Vec model\n",
    "# the skip-grams method is used here, with a window of 10\n",
    "model = gensim.models.Word2Vec(window=10, min_count=2, sg=1, workers=10)\n",
    "model.build_vocab(documents)  # prepare the model vocabulary\n",
    "\n",
    "# train model on available data\n",
    "# I use 5 epochs since that's standard\n",
    "model.train(corpus_iterable=documents, total_examples=len(documents), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7c498f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Word2Vec/subneighborhood_separated_articles/2014.csv')\n",
    "df = df.drop(['Unnamed: 0'], axis=1)\n",
    "df = df.fillna(\"('no article', 'no_id')\")\n",
    "df['hyde_park'] = df['hyde_park'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e11d66ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame(df['hyde_park'])\n",
    "sub_df.head()\n",
    "article_ids = []\n",
    "for row in sub_df.itertuples(index=False):\n",
    "    _, article_id = row.hyde_park\n",
    "    if article_id != 'no_id':\n",
    "        article_ids.append(article_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24dd9cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computation done only for hyde park articles from 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5844502b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting work with 2014_704\n",
      "\n",
      "2014_704 term weights sorted\n",
      "2014_704 similar words to most important terms generated\n",
      "\n",
      "starting work with 2014_243\n",
      "\n",
      "2014_243 term weights sorted\n",
      "2014_243 similar words to most important terms generated\n",
      "\n",
      "starting work with 2014_853\n",
      "\n",
      "2014_853 term weights sorted\n",
      "2014_853 similar words to most important terms generated\n",
      "\n",
      "starting work with 2014_892\n",
      "\n",
      "2014_892 term weights sorted\n",
      "2014_892 similar words to most important terms generated\n",
      "\n",
      "starting work with 2014_670\n",
      "\n",
      "2014_670 term weights sorted\n",
      "2014_670 similar words to most important terms generated\n",
      "\n",
      "starting work with 2014_81\n",
      "\n",
      "2014_81 term weights sorted\n",
      "2014_81 similar words to most important terms generated\n",
      "\n",
      "starting work with 2014_721\n",
      "\n",
      "2014_721 term weights sorted\n",
      "2014_721 similar words to most important terms generated\n",
      "\n",
      "starting work with 2014_969\n",
      "\n",
      "2014_969 term weights sorted\n",
      "2014_969 similar words to most important terms generated\n",
      "\n",
      "starting work with 2014_630\n",
      "\n",
      "2014_630 term weights sorted\n",
      "2014_630 similar words to most important terms generated\n",
      "\n",
      "starting work with 2014_1099\n",
      "\n",
      "2014_1099 term weights sorted\n",
      "2014_1099 similar words to most important terms generated\n",
      "\n",
      "starting work with 2014_899\n",
      "\n",
      "2014_899 term weights sorted\n",
      "2014_899 similar words to most important terms generated\n",
      "\n",
      "starting work with 2014_251\n",
      "\n",
      "2014_251 term weights sorted\n",
      "2014_251 similar words to most important terms generated\n",
      "\n",
      "starting work with 2014_1055\n",
      "\n",
      "2014_1055 term weights sorted\n",
      "2014_1055 similar words to most important terms generated\n",
      "\n",
      "starting work with 2014_356\n",
      "\n",
      "2014_356 term weights sorted\n",
      "2014_356 similar words to most important terms generated\n",
      "\n",
      "starting work with 2014_747\n",
      "\n",
      "2014_747 term weights sorted\n",
      "2014_747 similar words to most important terms generated\n",
      "\n",
      "starting work with 2014_839\n",
      "\n",
      "2014_839 term weights sorted\n",
      "2014_839 similar words to most important terms generated\n",
      "\n",
      "starting work with 2014_495\n",
      "\n",
      "2014_495 term weights sorted\n",
      "2014_495 similar words to most important terms generated\n",
      "\n",
      "starting work with 2014_1199\n",
      "\n",
      "2014_1199 term weights sorted\n",
      "2014_1199 similar words to most important terms generated\n",
      "\n",
      "starting work with 2014_1216\n",
      "\n",
      "2014_1216 term weights sorted\n",
      "2014_1216 similar words to most important terms generated\n",
      "\n",
      "starting work with 2014_100\n",
      "\n",
      "2014_100 term weights sorted\n",
      "2014_100 similar words to most important terms generated\n",
      "\n",
      "starting work with 2014_525\n",
      "\n",
      "2014_525 term weights sorted\n",
      "2014_525 similar words to most important terms generated\n",
      "\n",
      "starting work with 2014_1107\n",
      "\n",
      "2014_1107 term weights sorted\n",
      "2014_1107 similar words to most important terms generated\n",
      "\n",
      "starting work with 2014_83\n",
      "\n",
      "2014_83 term weights sorted\n",
      "2014_83 similar words to most important terms generated\n",
      "\n",
      "starting work with 2014_749\n",
      "\n",
      "2014_749 term weights sorted\n",
      "2014_749 similar words to most important terms generated\n",
      "\n",
      "starting work with 2014_1159\n",
      "\n",
      "2014_1159 term weights sorted\n",
      "2014_1159 similar words to most important terms generated\n",
      "\n",
      "starting work with 2014_654\n",
      "\n",
      "2014_654 term weights sorted\n",
      "2014_654 similar words to most important terms generated\n",
      "\n",
      "starting work with 2014_818\n",
      "\n",
      "2014_818 term weights sorted\n",
      "2014_818 similar words to most important terms generated\n",
      "\n",
      "starting work with 2014_490\n",
      "\n",
      "2014_490 term weights sorted\n",
      "2014_490 similar words to most important terms generated\n",
      "\n",
      "starting work with 2014_712\n",
      "\n",
      "2014_712 term weights sorted\n",
      "2014_712 similar words to most important terms generated\n",
      "\n",
      "starting work with 2014_798\n",
      "\n",
      "2014_798 term weights sorted\n",
      "2014_798 similar words to most important terms generated\n",
      "\n",
      "starting work with 2014_381\n",
      "\n",
      "2014_381 term weights sorted\n",
      "2014_381 similar words to most important terms generated\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for a_id in article_ids:\n",
    "    print('starting work with ' + a_id)\n",
    "    print()\n",
    "    a_id_TFIDF = pd.DataFrame()\n",
    "    for year in years:\n",
    "        data = pd.read_csv('../TF-IDF/Yearly_TFIDF_Scores_by_Subneighborhood/' + str(year) + '/hyde_park/TFIDF_' + a_id + '.csv')\n",
    "        data.columns = ['term', 'weight']\n",
    "        a_id_TFIDF = pd.concat([a_id_TFIDF, data], axis=0)\n",
    "\n",
    "    a_id_TFIDF = a_id_TFIDF.sort_values('weight', ascending=False)\n",
    "    print(a_id + ' term weights sorted')\n",
    "    \n",
    "    keywords = []\n",
    "    for row in a_id_TFIDF.itertuples(index=False):\n",
    "        if len(keywords) < 15 and row.term not in keywords:\n",
    "            if row.term != 'keefe' and row.term != 'dunkin' and row.term != 'audi' and row.term != 'incuding' and row.term != 'carty' and row.term != 'toole':# and row.term != 'beach' and row.term != 'dolli' and row.term != 'neck' and row.term != 'wednesday':\n",
    "                keywords.append(row.term)\n",
    "\n",
    "    # finding similar words and creating a csv file\n",
    "\n",
    "    def compute_similar_words(model,source_word, topn=5):\n",
    "        similar_words = [source_word]\n",
    "        try:\n",
    "            top_words = model.wv.most_similar(source_word, topn=topn)\n",
    "            similar_words.extend([val[0] for val in top_words])\n",
    "        except KeyError as err:\n",
    "            print(err.args)\n",
    "        return similar_words    \n",
    "\n",
    "    def compute_similar_words_for_all_tasks(model, topn=5):\n",
    "        columns = ['word' + str(i - 1) for i in range(1, topn + 2)]\n",
    "        df = pd.DataFrame(data=None, columns=columns)\n",
    "        for source_word in keywords:\n",
    "            similar_words = compute_similar_words(model, source_word, topn)\n",
    "            df.loc[len(df)] = similar_words\n",
    "        df.to_csv('similar_words_task/subneighborhood_TFIDF/hyde_park/' + a_id + '_similar_words.csv')\n",
    "    \n",
    "    words = compute_similar_words_for_all_tasks(model)\n",
    "    print(a_id + ' similar words to most important terms generated')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47f29d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff416ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
