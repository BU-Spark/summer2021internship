{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'south_boston_waterfront' not included\n",
    "subs = ['fenway', 'beacon_hill', 'downtown', 'south_boston', 'east_boston', 'back_bay', 'jamaica_plain',\n",
    "        'south_end', 'charlestown', 'brighton', 'allston', 'west_end', 'roslindale', 'north_end',\n",
    "        'mission_hill', 'harbor_islands', 'longwood_medical_area', 'dorchester', 'roxbury', 'mattapan', 'hyde_park']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus standardized\n",
      "\n",
      "starting work with fenway\n",
      "\n",
      "fenway term weights sorted\n",
      "fenway similar words to most important terms generated\n",
      "\n",
      "starting work with beacon_hill\n",
      "\n",
      "beacon_hill term weights sorted\n",
      "beacon_hill similar words to most important terms generated\n",
      "\n",
      "starting work with downtown\n",
      "\n",
      "downtown term weights sorted\n",
      "downtown similar words to most important terms generated\n",
      "\n",
      "starting work with south_boston\n",
      "\n",
      "south_boston term weights sorted\n",
      "south_boston similar words to most important terms generated\n",
      "\n",
      "starting work with east_boston\n",
      "\n",
      "east_boston term weights sorted\n",
      "east_boston similar words to most important terms generated\n",
      "\n",
      "starting work with back_bay\n",
      "\n",
      "back_bay term weights sorted\n",
      "back_bay similar words to most important terms generated\n",
      "\n",
      "starting work with jamaica_plain\n",
      "\n",
      "jamaica_plain term weights sorted\n",
      "jamaica_plain similar words to most important terms generated\n",
      "\n",
      "starting work with south_end\n",
      "\n",
      "south_end term weights sorted\n",
      "south_end similar words to most important terms generated\n",
      "\n",
      "starting work with charlestown\n",
      "\n",
      "charlestown term weights sorted\n",
      "charlestown similar words to most important terms generated\n",
      "\n",
      "starting work with brighton\n",
      "\n",
      "brighton term weights sorted\n",
      "brighton similar words to most important terms generated\n",
      "\n",
      "starting work with allston\n",
      "\n",
      "allston term weights sorted\n",
      "allston similar words to most important terms generated\n",
      "\n",
      "starting work with west_end\n",
      "\n",
      "west_end term weights sorted\n",
      "west_end similar words to most important terms generated\n",
      "\n",
      "starting work with roslindale\n",
      "\n",
      "roslindale term weights sorted\n",
      "roslindale similar words to most important terms generated\n",
      "\n",
      "starting work with north_end\n",
      "\n",
      "north_end term weights sorted\n",
      "north_end similar words to most important terms generated\n",
      "\n",
      "starting work with mission_hill\n",
      "\n",
      "mission_hill term weights sorted\n",
      "mission_hill similar words to most important terms generated\n",
      "\n",
      "starting work with harbor_islands\n",
      "\n",
      "harbor_islands term weights sorted\n",
      "harbor_islands similar words to most important terms generated\n",
      "\n",
      "starting work with longwood_medical_area\n",
      "\n",
      "longwood_medical_area term weights sorted\n",
      "longwood_medical_area similar words to most important terms generated\n",
      "\n",
      "(\"Key 'hokule' not present\",)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot set a row with mismatched columns",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-bc2bc694b460>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' similar words to most important terms generated'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_similar_words_for_all_tasks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-bc2bc694b460>\u001b[0m in \u001b[0;36mcompute_similar_words_for_all_tasks\u001b[1;34m(model, topn)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msource_word\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0msimilar_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_similar_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource_word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m             \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimilar_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'similar_words_task/subneighborhood_TFIDF/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msub\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_similar_words.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[0miloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"iloc\"\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m         \u001b[0miloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    671\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m   1624\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1625\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1626\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer_missing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1627\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer_missing\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m   1856\u001b[0m                     \u001b[1;31m# must have conforming columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1857\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1858\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cannot set a row with mismatched columns\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1859\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1860\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot set a row with mismatched columns"
     ]
    }
   ],
   "source": [
    "years = [2014, 2015, 2016, 2017, 2018]\n",
    "text_corpus = pd.DataFrame()\n",
    "for year in years:\n",
    "    temp = pd.read_csv('globe_data/bostonglobe' + str(year) + '.csv')\n",
    "    text_corpus = pd.concat([text_corpus, temp], axis=0)\n",
    "\n",
    "def custom_standardization(data):\n",
    "\n",
    "    spec_chars = [\"!\",'\"',\"#\",\"%\",\"&\",\"'\",\"(\",\")\", \"*\",\"+\",\",\",\n",
    "                      \"-\",\".\",\"/\",\":\",\";\",\"<\", \"=\",\">\",\"?\",\"@\",\"[\",\n",
    "                      \"\\\\\",\"]\",\"^\",\"_\", \"`\",\"{\",\"|\",\"}\",\"~\",\"–\", \n",
    "                      \"\\xc2\", \"\\xa0\", \"\\x80\", \"\\x9c\", \"\\x99\", \"\\x94\", \n",
    "                      \"\\xad\", \"\\xe2\", \"\\x9d\", \"\\n\", \"x9d\", \"xc2\", \n",
    "                      \"xa0\", \"x80\", \"x9c\", \"x99\", \"x94\", \"xad\", \"xe2\"]\n",
    "\n",
    "    for char in spec_chars:\n",
    "        data['text'] = data['text'].str.strip()\n",
    "        #data['text'] = str(data['text']).lower()\n",
    "        data['text'] = data['text'].str.replace(char, ' ')\n",
    "        #data['text'] = stemmer.stem(str(data['text']))\n",
    "\n",
    "    return data\n",
    "\n",
    "text_corpus = custom_standardization(text_corpus)\n",
    "print('corpus standardized')\n",
    "print()\n",
    "    \n",
    "# turn DataFrame into a list of lists of tokens\n",
    "documents = []\n",
    "for row in text_corpus.values:\n",
    "    [row] = row\n",
    "    temp = row.lower().split()\n",
    "    documents.append(temp)\n",
    "\n",
    "# create Word2Vec model\n",
    "# the skip-grams method is used here, with a window of 10\n",
    "model = gensim.models.Word2Vec(window=10, min_count=2, sg=1, workers=10)\n",
    "model.build_vocab(documents)  # prepare the model vocabulary\n",
    "\n",
    "# train model on available data\n",
    "# I use 5 epochs since that's standard\n",
    "model.train(corpus_iterable=documents, total_examples=len(documents), epochs=5)\n",
    "\n",
    "for sub in subs:\n",
    "    print('starting work with ' + sub)\n",
    "    print()\n",
    "    sub_TFIDF = pd.DataFrame()\n",
    "    for year in years:\n",
    "        data = pd.read_csv('../TF-IDF/Yearly_TFIDF_Scores_by_Neighborhood/' + str(year) + '/' + 'TFIDF_' + sub + '.csv')\n",
    "        data = data.drop(['Unnamed: 0'], axis=1)\n",
    "        sub_TFIDF = pd.concat([sub_TFIDF, data], axis=0)\n",
    "\n",
    "    sub_TFIDF = sub_TFIDF.sort_values('weight', ascending=False)\n",
    "    print(sub + ' term weights sorted')\n",
    "    \n",
    "    keywords = []\n",
    "    for row in sub_TFIDF.itertuples(index=False):\n",
    "        if len(keywords) < 15 and row.term not in keywords:\n",
    "            if row.term != 'hokule':\n",
    "                keywords.append(row.term)\n",
    "\n",
    "    # finding similar words and creating a csv file\n",
    "\n",
    "    def compute_similar_words(model,source_word, topn=5):\n",
    "        similar_words = [source_word]\n",
    "        try:\n",
    "            top_words = model.wv.most_similar(source_word, topn=topn)\n",
    "            similar_words.extend([val[0] for val in top_words])\n",
    "        except KeyError as err:\n",
    "            print(err.args)\n",
    "        return similar_words    \n",
    "\n",
    "    def compute_similar_words_for_all_tasks(model, topn=5):\n",
    "        columns = ['word' + str(i - 1) for i in range(1, topn + 2)]\n",
    "        df = pd.DataFrame(data=None, columns=columns)\n",
    "        for source_word in keywords:\n",
    "            similar_words = compute_similar_words(model, source_word, topn)\n",
    "            df.loc[len(df)] = similar_words\n",
    "        df.to_csv('similar_words_task/neighborhood_TFIDF/' + sub + '_similar_words.csv')\n",
    "    \n",
    "    words = compute_similar_words_for_all_tasks(model)\n",
    "    print(sub + ' similar words to most important terms generated')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allston: lots of names; a few different terms, but no clear pattern\n",
    "# back bay: a couple names; lots of words to do with biology/biotech\n",
    "# beacon hill: lots of names; one legal/law enforcement term\n",
    "# brighton: lots of names; one legal/law enforcement term\n",
    "# charlestown: no names; several terms to do with shapes; one legal/law enforcement term\n",
    "# downtown: lots of names; a couple legal/law enforcement terms\n",
    "# east boston: a couple names; a couple technical terms; no clear pattern\n",
    "# fenway: a few names; one political term; one legal/law enforcement term (debatable)\n",
    "# harbor islands: lots of names; several different terms, but no clear pattern\n",
    "# jamaica plain: no names; a lot of camping/sailing related terms\n",
    "# longwood medical center: almost all names\n",
    "# mission hill: almost all names\n",
    "# north end: several names; a couple legal/law enforcement terms\n",
    "# roslindale: several names; one legal/law enforcement related term; a couple sports related terms\n",
    "# south boston: a lot of names; a couple legal/law enforcement terms\n",
    "# south end: a lot of names; no clear pattern among the rest\n",
    "# west end: almost all names; one political term\n",
    "\n",
    "# dorchester: a couple names; one political term\n",
    "# hyde park: almost all names\n",
    "# mattapan: a couple names; a couple political terms; several sport (football) related terms\n",
    "# roxbury: several names; a couple political terms; a couple legal/law enforcement related terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trend across white neighborhoods: occurrence of terms to do with legal/law enforcement\n",
    "# trend across black neighborhoods: occurrence of terms to do with politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# potential next step: remove names from the corpus and repeat the entire process\n",
    "# concern: doing the exact same thing for Derry; currently struggling with removing names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# potential further step: compare the same word w.r.t. different neighborhoods \n",
    "# for example, generate word embeddings for the articles for each neighborhood and compare the word embeddings for a given term\n",
    "\n",
    "# lemmatize articles, remove stopwords, remove names from corpus, repeat entire process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can narrow down focus to work with individual articles\n",
    "# build word embedding on an article\n",
    "# look for certain keywords using some method and see how close those keywords are to their vectors from the entire corpus for that neighborhood (or perhaps, group of neighborhoods)\n",
    "\n",
    "# let's say you have an article talking about an election in dorchester\n",
    "# build a word embedding model on article\n",
    "# take keyword election\n",
    "# extract closest word to that keyword\n",
    "# take vector embedding for election and compare its vector with the\n",
    "    # rest of the articles for the neighborhood\n",
    "# this is essentially an extra step in case top TF-IDF words don't give enough information\n",
    "\n",
    "# also separate articles by sub-neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 cleaning the data (remove duplicates) DONE\n",
    "#2 separate black and white neighborhoods and then by sub-neighborhood DONE\n",
    "#3 work with a subset of the data\n",
    "#4 get TF-IDF weights for each article in that subset\n",
    "    #4.5 article:sub-neighborhood articles = neighborhood articles:text corpus\n",
    "#5 get word embeddings for the top words\n",
    "#6 get sub-neighborhood names from here: https://drive.google.com/file/d/1le8X9VQwO-cM4VVAAb4quAlgerr94T0x/view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
